{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "82db07aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "from enum import Enum\n",
    "from collections import deque, defaultdict, namedtuple\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "from datetime import datetime\n",
    "import math\n",
    "import copy\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "aa8aacbb",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Third-party library imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.optimize import linear_sum_assignment, minimize\n",
    "from scipy.stats import poisson, expon, gamma\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ed523ac9",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c9a6909a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def configure_tensorflow():\n",
    "    \"\"\"Configure TensorFlow to prevent memory issues\"\"\"\n",
    "    try:\n",
    "        gpus = tf.config.list_physical_devices('GPU')\n",
    "        if gpus:\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(\"TensorFlow configured successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"GPU configuration failed: {str(e)}\")\n",
    "        print(\"Continuing with CPU only\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cbbbef42",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "from enum import Enum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "15551775",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class CustomerType(Enum):\n",
    "    GENERAL = \"general\"\n",
    "    TECHNICAL = \"technical\"\n",
    "    BILLING = \"billing\"\n",
    "    SALES = \"sales\"\n",
    "    VIP = \"vip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6fa473f7",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class Priority(Enum):\n",
    "    LOW = 1\n",
    "    MEDIUM = 2\n",
    "    HIGH = 3\n",
    "    URGENT = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f7f68508",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class Complexity(Enum):\n",
    "    SIMPLE = 1\n",
    "    MODERATE = 2\n",
    "    COMPLEX = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6b69bf39",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Data Classes\n",
    "@dataclass\n",
    "class Customer:\n",
    "    id: str\n",
    "    type: CustomerType\n",
    "    priority: Priority\n",
    "    complexity: Complexity\n",
    "    arrival_time: float\n",
    "    skills_required: List[str]\n",
    "    expected_duration: float\n",
    "    language: str\n",
    "    patience: float\n",
    "    value: float\n",
    "    wait_time: float = 0.0\n",
    "    status: str = \"waiting\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "bf967f52",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Task:\n",
    "    id: str\n",
    "    required_skills: List[str]\n",
    "    priority: int\n",
    "    status: str = \"new\"\n",
    "    assigned_agent: Optional['Representative'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "69c19228",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Representative:\n",
    "    id: int\n",
    "    skills: Dict[str, float]\n",
    "    languages: List[str]\n",
    "    efficiency: float\n",
    "    max_concurrent: int\n",
    "    schedule: List[Tuple[float, float]]\n",
    "    cost_per_hour: float\n",
    "    current_load: int = 0\n",
    "    total_handled: int = 0\n",
    "    performance_metrics: Dict[str, float] = field(default_factory=dict)\n",
    "    \n",
    "    def is_available(self) -> bool:\n",
    "        return self.current_load < self.max_concurrent\n",
    "        \n",
    "    def assign_task(self, task: Task) -> None:\n",
    "        self.current_load += 1\n",
    "        task.assigned_agent = self\n",
    "        task.status = \"assigned\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e73d4a80",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class EnhancedQueueManager:\n",
    "    \"\"\"Enhanced queue management with predictive load balancing\"\"\"\n",
    "    def __init__(self, config: Dict):\n",
    "        self.config = config\n",
    "        self.queue_state = defaultdict(list)\n",
    "        self.waiting_customers = []\n",
    "        self.utilization_history = []\n",
    "        \n",
    "    def manage_queues(self, current_state: Dict) -> None:\n",
    "        \"\"\"Dynamic queue management with predictive balancing\"\"\"\n",
    "        arrival_rate = self._calculate_arrival_rate()\n",
    "        service_rate = self._calculate_service_rate()\n",
    "        \n",
    "        # Use Erlang C to determine the ideal staffing level.\n",
    "        required_staff = self._calculate_required_staff(\n",
    "            arrival_rate, \n",
    "            service_rate,\n",
    "            self.config['optimization_params']['target_wait_time']\n",
    "        )\n",
    "        \n",
    "       # Dynamically modify queue thresholds\n",
    "        self._adjust_thresholds(current_state, required_staff)\n",
    "        \n",
    "       # Adjust queues as necessary.\n",
    "        if self._needs_rebalancing(current_state):\n",
    "            self._rebalance_queues()\n",
    "\n",
    "    def _calculate_required_staff(self, arrival_rate: float, \n",
    "                                service_rate: float, \n",
    "                                target_wait: float) -> int:\n",
    "        \"\"\"Calculate required staff using Erlang C formula\"\"\"\n",
    "        traffic_intensity = arrival_rate / service_rate\n",
    "        \n",
    "        def erlang_c(n):\n",
    "            rho = traffic_intensity / n\n",
    "            if rho >= 1:\n",
    "                return float('inf')\n",
    "            \n",
    "            sum_term = sum((n * rho)**k / math.factorial(k) \n",
    "                          for k in range(n))\n",
    "            p0 = 1 / (sum_term + (n * rho)**n / \n",
    "                     (math.factorial(n) * (1 - rho)))\n",
    "            \n",
    "            # Determine the likelihood of waiting.\n",
    "            pw = ((n * rho)**n * p0) / (factorial(n) * (1 - rho))\n",
    "            \n",
    "            # Calculate expected wait time\n",
    "            w = pw / (n * service_rate * (1 - rho))\n",
    "            return w\n",
    "        \n",
    "       # Binary search for the bare minimum of employees\n",
    "        left, right = 1, 100\n",
    "        while left < right:\n",
    "            n = (left + right) // 2\n",
    "            if erlang_c(n) <= target_wait:\n",
    "                right = n\n",
    "            else:\n",
    "                left = n + 1\n",
    "        \n",
    "        return left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "66b431b9",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class AdaptiveLoadBalancer:\n",
    "    \"\"\"Adaptive load balancing with real-time adjustment\"\"\"\n",
    "    def __init__(self):\n",
    "        self.utilization_history = []\n",
    "        self.load_thresholds = {\n",
    "            'critical': 0.9,\n",
    "            'high': 0.8,\n",
    "            'normal': 0.7\n",
    "        }\n",
    "        \n",
    "    def balance_load(self, reps: List[Representative], \n",
    "                    current_state: Dict) -> Dict[int, float]:\n",
    "        \"\"\"Optimize load distribution\"\"\"\n",
    "        workload = self._calculate_workload(reps)\n",
    "        optimal_distribution = self._optimize_distribution(\n",
    "            workload, len(reps))\n",
    "        \n",
    "        return self._assign_workload(optimal_distribution, reps)\n",
    "    \n",
    "    def _optimize_distribution(self, workload: float, \n",
    "                             num_reps: int) -> np.ndarray:\n",
    "        \"\"\"Optimize workload distribution using linear programming\"\"\"\n",
    "        c = np.ones(num_reps)  # Cost vector\n",
    "        A = np.eye(num_reps)   # Constraint matrix\n",
    "        b = np.full(num_reps, workload/num_reps * 1.2)  # Upper bounds\n",
    "        \n",
    "        result = linprog(-c, A_ub=A, b_ub=b, \n",
    "                        bounds=(0, workload))\n",
    "        \n",
    "        return result.x if result.success else \\\n",
    "               np.full(num_reps, workload/num_reps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c0543130",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class PredictiveRouter:\n",
    "    \"\"\"Predictive routing with real-time adaptation\"\"\"\n",
    "    def __init__(self, config: Dict):\n",
    "        self.config = config\n",
    "        self.history = defaultdict(list)\n",
    "        self.model = self._build_prediction_model()\n",
    "    \n",
    "    def route_customer(self, customer: Customer,\n",
    "                      available_reps: List[Representative],\n",
    "                      current_state: Dict) -> Tuple[Representative, str]:\n",
    "        \"\"\"Route customer using predictive analytics\"\"\"\n",
    "        # Predict outcomes for each rep\n",
    "        predictions = []\n",
    "        for rep in available_reps:\n",
    "            if self._can_handle_customer(rep, customer):\n",
    "                pred = self._predict_outcome(customer, rep, current_state)\n",
    "                predictions.append((rep, pred))\n",
    "        \n",
    "        if not predictions:\n",
    "            return None, \"queued\"\n",
    "        \n",
    "        # Select best rep based on predictions\n",
    "        best_rep, _ = max(predictions, \n",
    "                         key=lambda x: self._calculate_score(x[1]))\n",
    "        \n",
    "        return best_rep, \"predicted_match\"\n",
    "    \n",
    "    def _predict_outcome(self, customer: Customer,\n",
    "                        rep: Representative,\n",
    "                        current_state: Dict) -> Dict:\n",
    "        \"\"\"Predict handling outcome\"\"\"\n",
    "        features = self._extract_features(customer, rep, current_state)\n",
    "        prediction = self.model.predict(features.reshape(1, -1))[0]\n",
    "        \n",
    "        return {\n",
    "            'wait_time': prediction[0],\n",
    "            'satisfaction': prediction[1],\n",
    "            'success_prob': prediction[2]\n",
    "        }\n",
    "    \n",
    "    def _calculate_score(self, prediction: Dict) -> float:\n",
    "        \"\"\"Calculate assignment score from prediction\"\"\"\n",
    "        weights = self.config['optimization_params']['weights']\n",
    "        \n",
    "        return (\n",
    "            weights['wait'] * (1 / (1 + prediction['wait_time'])) +\n",
    "            weights['satisfaction'] * prediction['satisfaction'] +\n",
    "            weights['success'] * prediction['success_prob']\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1f2db82f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class SyntheticDataGenerator:\n",
    "    def generate_customers(self, num_days):\n",
    "        \"\"\"Generate more realistic customer scenarios\"\"\"\n",
    "        customers = []\n",
    "        num_customers = num_days * 100  \n",
    "        \n",
    "        for i in range(num_customers):\n",
    "            # Generate realistic customer types\n",
    "            customer_type = random.choices(\n",
    "                list(CustomerType),\n",
    "                weights=[0.60, 0.20, 0.10, 0.05, 0.05]  \n",
    "            )[0]\n",
    "            \n",
    "            # Generate realistic priorities\n",
    "            priority = random.choices(\n",
    "                list(Priority),\n",
    "                weights=[0.40, 0.30, 0.20, 0.10]  # More low/medium priority\n",
    "            )[0]\n",
    "            \n",
    "            # Generate realistic patience levels\n",
    "            base_patience = {\n",
    "                Priority.URGENT: 120,\n",
    "                Priority.HIGH: 180,\n",
    "                Priority.MEDIUM: 300,\n",
    "                Priority.LOW: 420\n",
    "            }[priority]\n",
    "            patience = random.gauss(base_patience, base_patience * 0.2)\n",
    "            \n",
    "            # Construct a client with realistic qualities\n",
    "            customers.append(Customer(\n",
    "                id=f\"C{i}\",\n",
    "                type=customer_type,\n",
    "                priority=priority,\n",
    "                complexity=random.choice(list(Complexity)),\n",
    "                arrival_time=i * random.uniform(30, 90),  # Variable arrival times\n",
    "                skills_required=self._generate_skills(customer_type),\n",
    "                expected_duration=random.uniform(180, 600),  # 3-10 minutes\n",
    "                language=random.choices(['English', 'Spanish', 'French'], \n",
    "                                     weights=[0.7, 0.2, 0.1])[0],\n",
    "                patience=max(60, patience),  # Minimum 1 minute patience\n",
    "                value=random.uniform(40, 100)\n",
    "            ))\n",
    "        \n",
    "        return customers\n",
    "    \n",
    "    def _generate_skills(self, customer_type):\n",
    "        \"\"\"Generate realistic skill requirements\"\"\"\n",
    "        base_skills = {\n",
    "            CustomerType.GENERAL: ['GENERAL'],\n",
    "            CustomerType.TECHNICAL: ['TECHNICAL'],\n",
    "            CustomerType.BILLING: ['BILLING'],\n",
    "            CustomerType.SALES: ['SALES'],\n",
    "            CustomerType.VIP: ['VIP', 'GENERAL']\n",
    "        }[customer_type]\n",
    "        \n",
    "        # Sometimes add additional skills\n",
    "        if random.random() < 0.3:  # 30% chance\n",
    "            additional_skills = random.sample(\n",
    "                ['GENERAL', 'TECHNICAL', 'BILLING', 'SALES'],\n",
    "                k=random.randint(1, 2)\n",
    "            )\n",
    "            base_skills.extend(additional_skills)\n",
    "        \n",
    "        return list(set(base_skills))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "423242d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizationEngine:\n",
    "    def __init__(self, config: Dict):\n",
    "        self.config = config\n",
    "        self.discount_factor = config['optimization_params']['discount_factor']\n",
    "        self.value_function = self._initialize_value_function()\n",
    "        \n",
    "    def _initialize_value_function(self) -> tf.keras.Model:\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(64, activation='relu', input_shape=(10,)),\n",
    "            tf.keras.layers.Dropout(0.2),\n",
    "            tf.keras.layers.Dense(32, activation='relu'),\n",
    "            tf.keras.layers.Dense(16, activation='relu'),\n",
    "            tf.keras.layers.Dense(1)\n",
    "        ])\n",
    "        model.compile(optimizer='adam', loss='mse')\n",
    "        return model\n",
    "        \n",
    "    def optimize_assignment(self, \n",
    "                          customer: Customer,\n",
    "                          available_reps: List[Representative],\n",
    "                          current_state: Dict) -> Tuple[Representative, float]:\n",
    "        best_rep = None\n",
    "        best_score = float('-inf')\n",
    "        expected_wait = float('inf')\n",
    "        \n",
    "        for rep in available_reps:\n",
    "            if not self._can_handle_customer(rep, customer):\n",
    "                continue\n",
    "                \n",
    "            score, wait_time = self._calculate_assignment_score(\n",
    "                customer, rep, current_state)\n",
    "                \n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_rep = rep\n",
    "                expected_wait = wait_time\n",
    "                \n",
    "        return best_rep, expected_wait\n",
    "        \n",
    "    def _can_handle_customer(self, rep: Representative, \n",
    "                           customer: Customer) -> bool:\n",
    "        if rep.current_load >= rep.max_concurrent:\n",
    "            return False\n",
    "            \n",
    "        if not any(skill in rep.skills for skill in customer.skills_required):\n",
    "            return False\n",
    "            \n",
    "        if customer.language not in rep.languages:\n",
    "            return False\n",
    "            \n",
    "        return True\n",
    "        \n",
    "    def _calculate_assignment_score(self, \n",
    "                                  customer: Customer,\n",
    "                                  rep: Representative,\n",
    "                                  current_state: Dict) -> Tuple[float, float]:\n",
    "        immediate_cost = self._calculate_immediate_cost(\n",
    "            customer, rep, current_state)\n",
    "            \n",
    "        future_value = self._estimate_future_value(\n",
    "            customer, rep, current_state)\n",
    "            \n",
    "        wait_time = self._calculate_expected_wait(\n",
    "            customer, rep, current_state)\n",
    "            \n",
    "        total_value = immediate_cost + \\\n",
    "                     self.discount_factor * future_value\n",
    "                     \n",
    "        return total_value, wait_time\n",
    "        \n",
    "    def _calculate_immediate_cost(self,\n",
    "                                customer: Customer,\n",
    "                                rep: Representative,\n",
    "                                current_state: Dict) -> float:\n",
    "        base_cost = rep.cost_per_hour * \\\n",
    "                   (customer.expected_duration / 3600)\n",
    "                   \n",
    "        skill_factor = self._calculate_skill_factor(\n",
    "            customer.skills_required, rep.skills)\n",
    "            \n",
    "        urgency_cost = self._calculate_urgency_cost(customer)\n",
    "        \n",
    "        load_factor = rep.current_load / rep.max_concurrent\n",
    "        \n",
    "        return -(base_cost * (1 + load_factor) - \n",
    "                skill_factor + urgency_cost)\n",
    "        \n",
    "    def _calculate_skill_factor(self,\n",
    "                              required_skills: List[str],\n",
    "                              rep_skills: Dict[str, float]) -> float:\n",
    "        skill_scores = [\n",
    "            rep_skills.get(skill, 0) for skill in required_skills\n",
    "        ]\n",
    "        return np.mean(skill_scores) if skill_scores else 0\n",
    "        \n",
    "    def _calculate_urgency_cost(self, customer: Customer) -> float:\n",
    "        priority_weights = {\n",
    "            Priority.LOW: 1.0,\n",
    "            Priority.MEDIUM: 2.0,\n",
    "            Priority.HIGH: 4.0,\n",
    "            Priority.URGENT: 8.0\n",
    "        }\n",
    "        return priority_weights[customer.priority] * customer.value\n",
    "\n",
    "    def _calculate_expected_wait(self,\n",
    "                               customer: Customer,\n",
    "                               rep: Representative,\n",
    "                               current_state: Dict) -> float:\n",
    "        base_wait = customer.expected_duration / rep.efficiency\n",
    "        queue_factor = 1 + (current_state.get('queue_length', 0) * 0.1)\n",
    "        load_factor = 1 + (rep.current_load / rep.max_concurrent)\n",
    "        return base_wait * queue_factor * load_factor\n",
    "        \n",
    "    def _estimate_future_value(self,\n",
    "                             customer: Customer,\n",
    "                             rep: Representative,\n",
    "                             current_state: Dict) -> float:\n",
    "        state_features = self._extract_state_features(\n",
    "            customer, rep, current_state)\n",
    "        return float(self.value_function.predict(\n",
    "            state_features.reshape(1, -1))[0])\n",
    "        \n",
    "    def _extract_state_features(self,\n",
    "                              customer: Customer,\n",
    "                              rep: Representative,\n",
    "                              current_state: Dict) -> np.ndarray:\n",
    "        features = [\n",
    "            customer.value,\n",
    "            customer.priority.value,\n",
    "            customer.complexity.value,\n",
    "            len(customer.skills_required),\n",
    "            rep.efficiency,\n",
    "            rep.current_load / rep.max_concurrent,\n",
    "            current_state.get('queue_length', 0),\n",
    "            current_state.get('avg_wait_time', 0),\n",
    "            current_state.get('utilization', 0),\n",
    "            current_state.get('service_level', 0)\n",
    "        ]\n",
    "        return np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1d92e7dd",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class PerformanceTracker:\n",
    "    def __init__(self):\n",
    "        self.metrics = defaultdict(list)\n",
    "        self.current_state = {}\n",
    "        \n",
    "    def update_metrics(self,\n",
    "                      customer: Customer,\n",
    "                      rep: Optional[Representative],\n",
    "                      assignment_type: str) -> None:\n",
    "        self.metrics['assignments'].append({\n",
    "            'customer_id': customer.id,\n",
    "            'rep_id': rep.id if rep else None,\n",
    "            'type': assignment_type,\n",
    "            'timestamp': time.time()\n",
    "        })\n",
    "        \n",
    "    def get_current_state(self) -> Dict:\n",
    "        return self.current_state\n",
    "        \n",
    "    def update_state(self, new_state: Dict) -> None:\n",
    "        self.current_state.update(new_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4f2c78d3",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class RoutingEngine:\n",
    "    def __init__(self, config: Dict):\n",
    "        self.config = config\n",
    "        self.optimizer = OptimizationEngine(config)\n",
    "        \n",
    "    def route_customer(self,\n",
    "                      customer: Customer,\n",
    "                      available_reps: List[Representative],\n",
    "                      current_state: Dict) -> Tuple[Optional[Representative], str]:\n",
    "        \"\"\"Simplified, direct routing logic\"\"\"\n",
    "        # 1. Direct VIP/Urgent handling\n",
    "        if customer.type == CustomerType.VIP or customer.priority == Priority.URGENT:\n",
    "            rep = self._find_immediate_match(customer, available_reps)\n",
    "            if rep:\n",
    "                return rep, \"immediate\"\n",
    "        \n",
    "        # 2. Try standard assignment\n",
    "        best_rep = None\n",
    "        best_score = -1\n",
    "        \n",
    "        for rep in available_reps:\n",
    "            # Check basic availability\n",
    "            if rep.current_load >= rep.max_concurrent:\n",
    "                continue\n",
    "                \n",
    "            # Check language match\n",
    "            if customer.language not in rep.languages:\n",
    "                continue\n",
    "                \n",
    "            # Check skills match\n",
    "            skill_match = self._calculate_skill_match(customer, rep)\n",
    "            if skill_match < 0.6:  # Minimum skill threshold\n",
    "                continue\n",
    "                \n",
    "            # Calculate overall score\n",
    "            score = self._calculate_assignment_score(\n",
    "                skill_match, rep, customer)\n",
    "                \n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_rep = rep\n",
    "        \n",
    "        if best_rep:\n",
    "            return best_rep, \"assigned\"\n",
    "            \n",
    "        return None, \"queued\"\n",
    "    \n",
    "    def _find_immediate_match(self,\n",
    "                            customer: Customer,\n",
    "                            reps: List[Representative]) -> Optional[Representative]:\n",
    "        \"\"\"Find immediate match for priority customers\"\"\"\n",
    "        for rep in reps:\n",
    "            if rep.current_load >= rep.max_concurrent:\n",
    "                continue\n",
    "                \n",
    "            if customer.language not in rep.languages:\n",
    "                continue\n",
    "                \n",
    "            if any(skill in rep.skills for skill in customer.skills_required):\n",
    "                return rep\n",
    "        return None\n",
    "    \n",
    "    def _calculate_skill_match(self,\n",
    "                             customer: Customer,\n",
    "                             rep: Representative) -> float:\n",
    "        \"\"\"Calculate skill match score\"\"\"\n",
    "        matches = 0\n",
    "        total_skills = len(customer.skills_required)\n",
    "        \n",
    "        for skill in customer.skills_required:\n",
    "            if skill in rep.skills:\n",
    "                matches += rep.skills[skill]\n",
    "                \n",
    "        return matches / total_skills\n",
    "    \n",
    "    def _calculate_assignment_score(self,\n",
    "                                  skill_match: float,\n",
    "                                  rep: Representative,\n",
    "                                  customer: Customer) -> float:\n",
    "        \"\"\"Calculate final assignment score\"\"\"\n",
    "        load_score = 1 - (rep.current_load / rep.max_concurrent)\n",
    "        efficiency_score = rep.efficiency\n",
    "        priority_score = customer.priority.value / 4.0\n",
    "        \n",
    "        return (skill_match * 0.4 +\n",
    "                load_score * 0.3 +\n",
    "                efficiency_score * 0.2 +\n",
    "                priority_score * 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "889a4f0e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class AdvancedPredictor:\n",
    "    def __init__(self, config: Dict):\n",
    "        self.config = config\n",
    "        self.wait_time_model = self._build_wait_time_model()\n",
    "        self.abandonment_model = self._build_abandonment_model()\n",
    "        self.satisfaction_model = self._build_satisfaction_model()\n",
    "        self.workload_predictor = self._build_workload_predictor()\n",
    "        \n",
    "    def _build_wait_time_model(self) -> tf.keras.Model:\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(64, activation='relu', input_shape=(15,)),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.Dropout(0.3),\n",
    "            tf.keras.layers.Dense(32, activation='relu'),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.Dense(16, activation='relu'),\n",
    "            tf.keras.layers.Dense(1, activation='linear')\n",
    "        ])\n",
    "        model.compile(optimizer='adam', loss='huber')\n",
    "        return model\n",
    "\n",
    "    def _build_abandonment_model(self) -> tf.keras.Model:\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(32, activation='relu', input_shape=(10,)),\n",
    "            tf.keras.layers.Dropout(0.2),\n",
    "            tf.keras.layers.Dense(16, activation='relu'),\n",
    "            tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "        return model\n",
    "\n",
    "    def _build_satisfaction_model(self) -> tf.keras.Model:\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(32, activation='relu', input_shape=(12,)),\n",
    "            tf.keras.layers.Dropout(0.2),\n",
    "            tf.keras.layers.Dense(16, activation='relu'),\n",
    "            tf.keras.layers.Dense(1, activation='linear')\n",
    "        ])\n",
    "        model.compile(optimizer='adam', loss='mse')\n",
    "        return model\n",
    "\n",
    "    def _build_workload_predictor(self) -> tf.keras.Model:\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(64, activation='relu', input_shape=(20,)),\n",
    "            tf.keras.layers.Dropout(0.3),\n",
    "            tf.keras.layers.Dense(32, activation='relu'),\n",
    "            tf.keras.layers.Dense(16, activation='relu'),\n",
    "            tf.keras.layers.Dense(8, activation='linear')\n",
    "        ])\n",
    "        model.compile(optimizer='adam', loss='mse')\n",
    "        return model\n",
    "\n",
    "    def predict_comprehensive_metrics(self,\n",
    "                                   customer: Customer,\n",
    "                                   rep: Representative,\n",
    "                                   current_state: Dict) -> Dict:\n",
    "        wait_time_features = self._extract_wait_time_features(\n",
    "            customer, rep, current_state)\n",
    "        abandonment_features = self._extract_abandonment_features(\n",
    "            customer, current_state)\n",
    "        satisfaction_features = self._extract_satisfaction_features(\n",
    "            customer, rep, current_state)\n",
    "        workload_features = self._extract_workload_features(\n",
    "            current_state)\n",
    "\n",
    "        predictions = {\n",
    "            'wait_time': {\n",
    "                'mean': float(self.wait_time_model.predict(wait_time_features)[0][0]),\n",
    "                'lower': 0,\n",
    "                'upper': 0\n",
    "            },\n",
    "            'abandonment_prob': {\n",
    "                'mean': float(self.abandonment_model.predict(abandonment_features)[0][0]),\n",
    "                'lower': 0,\n",
    "                'upper': 0\n",
    "            },\n",
    "            'satisfaction_score': float(self.satisfaction_model.predict(satisfaction_features)[0][0]),\n",
    "            'workload_forecast': self.workload_predictor.predict(workload_features).tolist()\n",
    "        }\n",
    "\n",
    "        return predictions\n",
    "\n",
    "    def _extract_wait_time_features(self,\n",
    "                                  customer: Customer,\n",
    "                                  rep: Representative,\n",
    "                                  current_state: Dict) -> np.ndarray:\n",
    "        features = [\n",
    "            customer.priority.value,\n",
    "            customer.complexity.value,\n",
    "            customer.value,\n",
    "            customer.expected_duration,\n",
    "            rep.current_load,\n",
    "            rep.efficiency,\n",
    "            current_state.get('queue_length', 0),\n",
    "            current_state.get('avg_wait_time', 0),\n",
    "            current_state.get('utilization', 0),\n",
    "            current_state.get('service_level', 0),\n",
    "            len(customer.skills_required),\n",
    "            rep.max_concurrent,\n",
    "            time.time() % 86400 / 3600,\n",
    "            len(rep.skills),\n",
    "            float(customer.type.value == 'vip')\n",
    "        ]\n",
    "        return np.array(features).reshape(1, -1)\n",
    "\n",
    "    def _extract_abandonment_features(self,\n",
    "                                    customer: Customer,\n",
    "                                    current_state: Dict) -> np.ndarray:\n",
    "        features = [\n",
    "            customer.priority.value,\n",
    "            customer.patience,\n",
    "            customer.value,\n",
    "            current_state.get('queue_length', 0),\n",
    "            current_state.get('avg_wait_time', 0),\n",
    "            current_state.get('utilization', 0),\n",
    "            current_state.get('service_level', 0),\n",
    "            len(customer.skills_required),\n",
    "            time.time() % 86400 / 3600,\n",
    "            float(customer.type.value == 'vip')\n",
    "        ]\n",
    "        return np.array(features).reshape(1, -1)\n",
    "\n",
    "    def _extract_satisfaction_features(self,\n",
    "                                    customer: Customer,\n",
    "                                    rep: Representative,\n",
    "                                    current_state: Dict) -> np.ndarray:\n",
    "        features = [\n",
    "            customer.priority.value,\n",
    "            customer.complexity.value,\n",
    "            customer.value,\n",
    "            customer.expected_duration,\n",
    "            rep.efficiency,\n",
    "            rep.current_load / rep.max_concurrent,\n",
    "            current_state.get('queue_length', 0),\n",
    "            current_state.get('avg_wait_time', 0),\n",
    "            current_state.get('service_level', 0),\n",
    "            len(customer.skills_required),\n",
    "            time.time() % 86400 / 3600,\n",
    "            float(customer.type.value == 'vip')\n",
    "        ]\n",
    "        return np.array(features).reshape(1, -1)\n",
    "\n",
    "    def _extract_workload_features(self, current_state: Dict) -> np.ndarray:\n",
    "        features = np.zeros(20)\n",
    "        features[0] = current_state.get('queue_length', 0)\n",
    "        features[1] = current_state.get('avg_wait_time', 0)\n",
    "        features[2] = current_state.get('utilization', 0)\n",
    "        features[3] = current_state.get('service_level', 0)\n",
    "        features[4] = time.time() % 86400 / 3600\n",
    "        return features.reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "71aa0908",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class RoutingVisualizer:\n",
    "    def __init__(self):\n",
    "        self.fig_size = (12, 8)\n",
    "        try:\n",
    "            plt.style.use('seaborn')\n",
    "            # Configure dark grid manually\n",
    "            plt.rcParams['axes.grid'] = True\n",
    "            plt.rcParams['grid.color'] = '0.8'\n",
    "            plt.rcParams['grid.linestyle'] = '--'\n",
    "            plt.rcParams['axes.facecolor'] = '#f0f0f0'\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not set preferred style: {str(e)}\")\n",
    "            # Fallback to basic style\n",
    "            plt.style.use('default')\n",
    "        \n",
    "    def create_dashboard(self, \n",
    "                        current_state: Dict,\n",
    "                        predictions: Dict,\n",
    "                        history: Dict) -> None:\n",
    "        fig = plt.figure(figsize=(15, 10))\n",
    "        gs = fig.add_gridspec(3, 2)\n",
    "        \n",
    "        # Create subplots with error handling\n",
    "        try:\n",
    "            self._plot_wait_time_distribution(\n",
    "                fig.add_subplot(gs[0, 0]), \n",
    "                history.get('waiting_times', [])\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not plot wait time distribution: {str(e)}\")\n",
    "            \n",
    "        try:\n",
    "            self._plot_service_level_trend(\n",
    "                fig.add_subplot(gs[0, 1]), \n",
    "                history.get('service_levels', [])\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not plot service level trend: {str(e)}\")\n",
    "            \n",
    "        try:\n",
    "            self._plot_rep_utilization(\n",
    "                fig.add_subplot(gs[1, 0]), \n",
    "                current_state.get('rep_utilization', {})\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not plot rep utilization: {str(e)}\")\n",
    "            \n",
    "        try:\n",
    "            self._plot_queue_prediction(\n",
    "                fig.add_subplot(gs[1, 1]), \n",
    "                predictions.get('queue_forecast', [])\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not plot queue prediction: {str(e)}\")\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        try:\n",
    "            plt.savefig('routing_dashboard.png', dpi=300, bbox_inches='tight')\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not save dashboard: {str(e)}\")\n",
    "        \n",
    "    def _plot_wait_time_distribution(self, \n",
    "                                   ax: plt.Axes, \n",
    "                                   wait_times: List[float]) -> None:\n",
    "        if not wait_times:\n",
    "            ax.text(0.5, 0.5, 'No data available', \n",
    "                   horizontalalignment='center',\n",
    "                   verticalalignment='center')\n",
    "            ax.set_title('Wait Time Distribution')\n",
    "            return\n",
    "            \n",
    "        sns.histplot(wait_times, kde=True, ax=ax)\n",
    "        mean_wait = np.mean(wait_times)\n",
    "        percentile_95 = np.percentile(wait_times, 95)\n",
    "        \n",
    "        ax.axvline(mean_wait, color='r', linestyle='--',\n",
    "                  label=f'Mean: {mean_wait:.1f}s')\n",
    "        ax.axvline(percentile_95, color='g', linestyle=':',\n",
    "                  label='95th percentile')\n",
    "        ax.set_title('Wait Time Distribution')\n",
    "        ax.set_xlabel('Wait Time (seconds)')\n",
    "        ax.legend()\n",
    "        \n",
    "    def _plot_service_level_trend(self, \n",
    "                                ax: plt.Axes,\n",
    "                                service_levels: List[float]) -> None:\n",
    "        if not service_levels:\n",
    "            ax.text(0.5, 0.5, 'No data available', \n",
    "                   horizontalalignment='center',\n",
    "                   verticalalignment='center')\n",
    "            ax.set_title('Service Level Trend')\n",
    "            return\n",
    "            \n",
    "        times = range(len(service_levels))\n",
    "        ax.plot(times, service_levels, label='Service Level')\n",
    "        \n",
    "        # Only calculate trend if we have enough data points\n",
    "        if len(service_levels) > 1:\n",
    "            z = np.polyfit(times, service_levels, 1)\n",
    "            p = np.poly1d(z)\n",
    "            ax.plot(times, p(times), \"r--\", label='Trend')\n",
    "            \n",
    "            std = np.std(service_levels)\n",
    "            ax.fill_between(times, \n",
    "                           np.array(service_levels) - std,\n",
    "                           np.array(service_levels) + std,\n",
    "                           alpha=0.2)\n",
    "        \n",
    "        ax.axhline(y=0.8, color='g', linestyle=':', label='Target (80%)')\n",
    "        ax.set_title('Service Level Trend')\n",
    "        ax.set_xlabel('Time Period')\n",
    "        ax.set_ylabel('Service Level')\n",
    "        ax.legend()\n",
    "        \n",
    "    def _plot_rep_utilization(self, \n",
    "                            ax: plt.Axes,\n",
    "                            utilization_data: Dict) -> None:\n",
    "        if not utilization_data:\n",
    "            ax.text(0.5, 0.5, 'No data available', \n",
    "                   horizontalalignment='center',\n",
    "                   verticalalignment='center')\n",
    "            ax.set_title('Representative Utilization')\n",
    "            return\n",
    "            \n",
    "        utilization_matrix = []\n",
    "        for rep_id, data in utilization_data.items():\n",
    "            if 'hourly_utilization' in data:\n",
    "                utilization_matrix.append(data['hourly_utilization'])\n",
    "        \n",
    "        if utilization_matrix:\n",
    "            sns.heatmap(utilization_matrix, \n",
    "                       cmap='YlOrRd',\n",
    "                       ax=ax,\n",
    "                       cbar_kws={'label': 'Utilization %'},\n",
    "                       xticklabels=range(24),\n",
    "                       yticklabels=[f'Rep {i}' for i in utilization_data.keys()])\n",
    "            \n",
    "            ax.set_title('Representative Utilization by Hour')\n",
    "            ax.set_xlabel('Hour of Day')\n",
    "        else:\n",
    "            ax.text(0.5, 0.5, 'No utilization data available', \n",
    "                   horizontalalignment='center',\n",
    "                   verticalalignment='center')\n",
    "        \n",
    "    def _plot_queue_prediction(self, \n",
    "                             ax: plt.Axes,\n",
    "                             forecast_data: List[float]) -> None:\n",
    "        if not forecast_data:\n",
    "            ax.text(0.5, 0.5, 'No forecast data available', \n",
    "                   horizontalalignment='center',\n",
    "                   verticalalignment='center')\n",
    "            ax.set_title('Queue Length Forecast')\n",
    "            return\n",
    "            \n",
    "        times = range(len(forecast_data))\n",
    "        ax.plot(times, forecast_data, label='Forecast')\n",
    "        \n",
    "        # Add confidence band\n",
    "        ax.fill_between(times,\n",
    "                       np.array(forecast_data) * 0.8,\n",
    "                       np.array(forecast_data) * 1.2,\n",
    "                       alpha=0.2)\n",
    "        ax.set_title('Queue Length Forecast')\n",
    "        ax.set_xlabel('Time Period')\n",
    "        ax.set_ylabel('Queue Length')\n",
    "        ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4dfa58ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerformanceAnalyzer:\n",
    "    def __init__(self):\n",
    "        self.metrics = {}\n",
    "        \n",
    "    def analyze_performance(self, results: Dict) -> Dict:\n",
    "        return {\n",
    "            'Routing Efficiency': f\"{self._calculate_routing_efficiency(results):.2f}%\",\n",
    "            'Queue Optimization': f\"{self._calculate_queue_efficiency(results):.2f}%\",\n",
    "            'Skill Utilization': f\"{self._calculate_skill_utilization(results):.2f}%\",\n",
    "            'Cost Efficiency': f\"${self._calculate_cost_efficiency(results):.2f}/call\"\n",
    "        }\n",
    "        \n",
    "    def _calculate_routing_efficiency(self, results: Dict) -> float:\n",
    "        if not results.get('waiting_times'):\n",
    "            return 0.0\n",
    "        immediate = sum(1 for wt in results['waiting_times'] if wt == 0)\n",
    "        return immediate / len(results['waiting_times']) * 100\n",
    "        \n",
    "    def _calculate_queue_efficiency(self, results: Dict) -> float:\n",
    "        if not results.get('waiting_times'):\n",
    "            return 0.0\n",
    "        long_waits = sum(1 for wt in results['waiting_times'] if wt > 300)\n",
    "        return (1 - long_waits / len(results['waiting_times'])) * 100\n",
    "        \n",
    "    def _calculate_skill_utilization(self, results: Dict) -> float:\n",
    "        if not results.get('rep_utilization'):\n",
    "            return 0.0\n",
    "        return np.mean(results['rep_utilization']) * 100\n",
    "        \n",
    "    def _calculate_cost_efficiency(self, results: Dict) -> float:\n",
    "        if not results.get('total_calls') or results['total_calls'] == 0:\n",
    "            return 0.0\n",
    "        total_cost = sum(results.get('costs', [0]))\n",
    "        return total_cost / results['total_calls']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "15f9c971",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class ImprovedQueueManager:\n",
    "    def __init__(self):\n",
    "        self.priority_queue = []\n",
    "        self.regular_queue = []\n",
    "        self.last_rebalance = time.time()\n",
    "        \n",
    "    def add_customer(self, customer: Customer) -> None:\n",
    "        if self._is_priority_customer(customer):\n",
    "            heapq.heappush(\n",
    "                self.priority_queue,\n",
    "                (-customer.priority.value, -customer.value, customer)\n",
    "            )\n",
    "        else:\n",
    "            heapq.heappush(\n",
    "                self.regular_queue,\n",
    "                (-customer.priority.value, customer.wait_time, customer)\n",
    "            )\n",
    "    \n",
    "    def get_next_customer(self) -> Optional[Customer]:\n",
    "        # Always check priority queue first\n",
    "        if self.priority_queue:\n",
    "            return heapq.heappop(self.priority_queue)[2]\n",
    "        if self.regular_queue:\n",
    "            return heapq.heappop(self.regular_queue)[2]\n",
    "        return None\n",
    "    \n",
    "    def rebalance_queues(self, current_state: Dict) -> None:\n",
    "        \"\"\"Rebalance queues based on wait times and priorities\"\"\"\n",
    "        current_time = time.time()\n",
    "        \n",
    "        # Update wait times and reprioritize\n",
    "        self._update_wait_times(current_time)\n",
    "        self._reprioritize_queues(current_state)\n",
    "        \n",
    "        # Promote customers who have waited too long\n",
    "        self._promote_long_waiters(current_time)\n",
    "        \n",
    "        self.last_rebalance = current_time\n",
    "    \n",
    "    def _is_priority_customer(self, customer: Customer) -> bool:\n",
    "        return (customer.type == CustomerType.VIP or \n",
    "                customer.priority in [Priority.URGENT, Priority.HIGH])\n",
    "    \n",
    "    def _update_wait_times(self, current_time: float) -> None:\n",
    "        for queue in [self.priority_queue, self.regular_queue]:\n",
    "            for _, _, customer in queue:\n",
    "                customer.wait_time = current_time - customer.arrival_time\n",
    "    \n",
    "    def _reprioritize_queues(self, current_state: Dict) -> None:\n",
    "        \"\"\"Reprioritize based on wait times and system load\"\"\"\n",
    "        utilization = current_state.get('utilization', 0)\n",
    "        \n",
    "        if utilization > 0.9:  # High load\n",
    "            self._aggressive_reprioritization()\n",
    "        elif utilization > 0.7:  # Moderate load\n",
    "            self._balanced_reprioritization()\n",
    "        else:  # Normal load\n",
    "            self._normal_reprioritization()\n",
    "    \n",
    "    def _promote_long_waiters(self, current_time: float) -> None:\n",
    "        \"\"\"Promote customers who have waited too long\"\"\"\n",
    "        promoted = []\n",
    "        \n",
    "        for _, _, customer in self.regular_queue:\n",
    "            wait_time = current_time - customer.arrival_time\n",
    "            max_wait = self._get_max_wait_time(customer)\n",
    "            \n",
    "            if wait_time > max_wait * 0.8:  # Promote before hitting max wait\n",
    "                promoted.append(customer)\n",
    "        \n",
    "        for customer in promoted:\n",
    "            self.priority_queue.append(\n",
    "                (-customer.priority.value - 1,  # Boost priority\n",
    "                 -customer.value,\n",
    "                 customer)\n",
    "            )\n",
    "class QValueNetwork:\n",
    "    \"\"\"Deep Q-Network for learning optimal routing policies\"\"\"\n",
    "    def __init__(self, state_dim: int, action_dim: int):\n",
    "        self.model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(128, activation='relu', input_shape=(state_dim,)),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.Dropout(0.2),\n",
    "            tf.keras.layers.Dense(64, activation='relu'),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.Dense(32, activation='relu'),\n",
    "            tf.keras.layers.Dense(action_dim)\n",
    "        ])\n",
    "        self.model.compile(optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "                         loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "fbb569e7",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class RoutingQLearningAgent:\n",
    "    \"\"\"Q-Learning based routing optimization with improved memory management\"\"\"\n",
    "    def __init__(self, state_dim: int, action_dim: int, config: Dict):\n",
    "        self.q_network = QValueNetwork(state_dim, action_dim)\n",
    "        self.target_network = QValueNetwork(state_dim, action_dim)\n",
    "        self.memory = deque(maxlen=10000)  # Fixed size replay buffer\n",
    "        self.gamma = 0.95\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.batch_size = 32\n",
    "        self.update_target_freq = 100\n",
    "        self.step_counter = 0\n",
    "        self.training_iterations = 0\n",
    "        \n",
    "    def store_experience(self, \n",
    "                        state: np.ndarray, \n",
    "                        action: int,\n",
    "                        reward: float,\n",
    "                        next_state: np.ndarray,\n",
    "                        done: bool) -> None:\n",
    "        \"\"\"Safely store experience in replay buffer\"\"\"\n",
    "        try:\n",
    "            self.memory.append((\n",
    "                state.copy(),  # Make copies to prevent reference issues\n",
    "                action,\n",
    "                reward,\n",
    "                next_state.copy(),\n",
    "                done\n",
    "            ))\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to store experience: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "fdfb5c23",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def prepare_training_batch(memory: deque, batch_size: int) -> Tuple[np.ndarray, ...]:\n",
    "    \"\"\"Prepare batch for Q-learning training with error handling\"\"\"\n",
    "    try:\n",
    "        if len(memory) < batch_size:\n",
    "            raise ValueError(f\"Not enough samples in memory. Have {len(memory)}, need {batch_size}\")\n",
    "            \n",
    "        batch = random.sample(list(memory), batch_size)\n",
    "        \n",
    "        # Safely extract and stack batch components\n",
    "        states = np.vstack([x[0] for x in batch])\n",
    "        actions = np.array([x[1] for x in batch])\n",
    "        rewards = np.array([x[2] for x in batch])\n",
    "        next_states = np.vstack([x[3] for x in batch])\n",
    "        dones = np.array([x[4] for x in batch])\n",
    "        \n",
    "        return states, actions, rewards, next_states, dones\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error preparing training batch: {str(e)}\")\n",
    "        # Return zero arrays of correct shape as fallback\n",
    "        state_shape = memory[0][0].shape[0] if memory else 1\n",
    "        return (\n",
    "            np.zeros((batch_size, state_shape)),\n",
    "            np.zeros(batch_size),\n",
    "            np.zeros(batch_size),\n",
    "            np.zeros((batch_size, state_shape)),\n",
    "            np.zeros(batch_size)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "920cdfc1",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class EnhancedOptimizationEngine:\n",
    "    \"\"\"Enhanced optimization engine with multiple strategies\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Dict):\n",
    "        state_dim = 15  # State features dimension\n",
    "        action_dim = len(config['representatives'])\n",
    "        self.q_agent = RoutingQLearningAgent(state_dim, action_dim, config)\n",
    "        self.mmk_queue = MMKQueue()  # M/M/K queuing model\n",
    "        self.load_balancer = AdaptiveLoadBalancer()\n",
    "        \n",
    "    def optimize_assignment(self,\n",
    "                          customer: Customer,\n",
    "                          available_reps: List[Representative],\n",
    "                          current_state: Dict) -> Tuple[Representative, float]:\n",
    "        # Extract state features\n",
    "        state = self._extract_state_features(customer, available_reps, current_state)\n",
    "        \n",
    "        # Get available actions (representatives)\n",
    "        available_actions = [i for i, rep in enumerate(available_reps)\n",
    "                           if self._can_handle_customer(rep, customer)]\n",
    "        \n",
    "        if not available_actions:\n",
    "            return None, float('inf')\n",
    "        \n",
    "        # Get action from Q-learning agent\n",
    "        action = self.q_agent.get_action(state, available_actions)\n",
    "        best_rep = available_reps[action]\n",
    "        \n",
    "        # Calculate expected wait time using M/M/K model\n",
    "        expected_wait = self.mmk_queue.calculate_wait_time(\n",
    "            arrival_rate=current_state.get('arrival_rate', 0.1),\n",
    "            service_rate=1/customer.expected_duration,\n",
    "            num_servers=len(available_reps),\n",
    "            utilization=current_state.get('utilization', 0)\n",
    "        )\n",
    "        \n",
    "        # Update load balancing weights\n",
    "        self.load_balancer.update_weights(\n",
    "            current_state['rep_utilization'],\n",
    "            expected_wait\n",
    "        )\n",
    "        \n",
    "        return best_rep, expected_wait\n",
    "        \n",
    "    def _extract_state_features(self,\n",
    "                              customer: Customer,\n",
    "                              available_reps: List[Representative],\n",
    "                              current_state: Dict) -> np.ndarray:\n",
    "        features = [\n",
    "            customer.priority.value / 4,  # Normalize priority\n",
    "            customer.complexity.value / 3,  # Normalize complexity\n",
    "            customer.value / 100,  # Normalize customer value\n",
    "            customer.expected_duration / 3600,  # Convert to hours\n",
    "            current_state.get('queue_length', 0) / 100,  # Normalize queue length\n",
    "            current_state.get('avg_wait_time', 0) / 300,  # Normalize wait time\n",
    "            current_state.get('utilization', 0),\n",
    "            current_state.get('service_level', 0),\n",
    "            len(customer.skills_required) / 5,  # Normalize skill count\n",
    "            float(customer.type == CustomerType.VIP),\n",
    "            self.load_balancer.get_current_load_factor(),\n",
    "            self.mmk_queue.get_current_utilization(),\n",
    "            time.time() % 86400 / 86400,  # Time of day normalized\n",
    "            self.load_balancer.get_imbalance_factor(),\n",
    "            float(len(available_reps)) / 10  # Normalize available reps\n",
    "        ]\n",
    "        return np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "043ce719",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class MMKQueue:\n",
    "    \"\"\"M/M/K queuing model for wait time estimation\"\"\"\n",
    "    \n",
    "    def calculate_wait_time(self,\n",
    "                          arrival_rate: float,\n",
    "                          service_rate: float,\n",
    "                          num_servers: int,\n",
    "                          utilization: float) -> float:\n",
    "        rho = arrival_rate / (service_rate * num_servers)  # Traffic intensity\n",
    "        \n",
    "        if rho >= 1:\n",
    "            return float('inf')\n",
    "        \n",
    "        # Calculate P0 (probability of empty system)\n",
    "        sum_term = sum([(num_servers * rho)**n / math.factorial(n) \n",
    "                       for n in range(num_servers)])\n",
    "        last_term = (num_servers * rho)**num_servers / \\\n",
    "                   (math.factorial(num_servers) * (1 - rho))\n",
    "        p0 = 1 / (sum_term + last_term)\n",
    "        \n",
    "        # Calculate Lq (expected queue length)\n",
    "        lq = (p0 * (num_servers * rho)**num_servers * rho) / \\\n",
    "             (math.factorial(num_servers) * (1 - rho)**2)\n",
    "        \n",
    "        # Use Little's Law to determine the anticipated wait time.\n",
    "        wq = lq / arrival_rate\n",
    "        \n",
    "        return wq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d6c0db69",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class AdaptiveLoadBalancer:\n",
    "    \"\"\"Adaptive load balancing with dynamic weights\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.weights = defaultdict(lambda: 1.0)\n",
    "        self.utilization_history = []\n",
    "        self.wait_time_history = []\n",
    "        \n",
    "    def update_weights(self,\n",
    "                      utilization: Dict[int, float],\n",
    "                      current_wait: float) -> None:\n",
    "        self.utilization_history.append(utilization)\n",
    "        self.wait_time_history.append(current_wait)\n",
    "        \n",
    "        if len(self.utilization_history) > 100:\n",
    "            self.utilization_history.pop(0)\n",
    "            self.wait_time_history.pop(0)\n",
    "        \n",
    "        # Update weights based on recent performance\n",
    "        for rep_id, util in utilization.items():\n",
    "            efficiency = self._calculate_efficiency(rep_id)\n",
    "            self.weights[rep_id] = 1 / (util * (1 + efficiency))\n",
    "    \n",
    "    def get_current_load_factor(self) -> float:\n",
    "        if not self.utilization_history:\n",
    "            return 0.0\n",
    "        return np.mean([np.mean(list(util.values())) \n",
    "                       for util in self.utilization_history[-10:]])\n",
    "    \n",
    "    def get_imbalance_factor(self) -> float:\n",
    "        if not self.utilization_history:\n",
    "            return 0.0\n",
    "        recent_utils = [list(util.values()) \n",
    "                       for util in self.utilization_history[-10:]]\n",
    "        return np.mean([np.std(utils) for utils in recent_utils])\n",
    "    \n",
    "    def _calculate_efficiency(self, rep_id: int) -> float:\n",
    "        if not self.utilization_history:\n",
    "            return 0.0\n",
    "        \n",
    "        rep_utils = [util[rep_id] \n",
    "                    for util in self.utilization_history[-10:]\n",
    "                    if rep_id in util]\n",
    "        \n",
    "        if not rep_utils:\n",
    "            return 0.0\n",
    "            \n",
    "        return np.mean(rep_utils) * \\\n",
    "               (1 + np.std(rep_utils))  # Penalize variability    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6dd5898f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class CoreRoutingEngine:\n",
    "    \"\"\"Core routing engine with immediate handling capacity\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Dict):\n",
    "        self.config = config\n",
    "        self.last_rebalance = time.time()\n",
    "        self.current_load = defaultdict(int)\n",
    "        \n",
    "    def route_customer(self,\n",
    "                      customer: Customer,\n",
    "                      available_reps: List[Representative],\n",
    "                      current_state: Dict) -> Tuple[Optional[Representative], str]:\n",
    "        \"\"\"Direct routing with immediate handling\"\"\"\n",
    "        \n",
    "        # 1. Immediate VIP/High Priority Handling\n",
    "        if self._is_priority_customer(customer):\n",
    "            rep = self._find_priority_representative(customer, available_reps)\n",
    "            if rep:\n",
    "                return rep, \"priority_assigned\"\n",
    "        \n",
    "        # 2. Try Standard Immediate Assignment\n",
    "        rep = self._find_available_representative(customer, available_reps)\n",
    "        if rep:\n",
    "            return rep, \"immediate_assigned\"\n",
    "        \n",
    "        # 3. Load Balancing Check\n",
    "        if self._should_rebalance(current_state):\n",
    "            self._rebalance_loads(available_reps)\n",
    "        \n",
    "        # 4. Try After Rebalancing\n",
    "        rep = self._find_available_representative(customer, available_reps)\n",
    "        if rep:\n",
    "            return rep, \"rebalanced_assigned\"\n",
    "        \n",
    "        # 5. Emergency Handling\n",
    "        if self._is_system_overloaded(current_state):\n",
    "            rep = self._emergency_assignment(customer, available_reps)\n",
    "            if rep:\n",
    "                return rep, \"emergency_assigned\"\n",
    "        \n",
    "        return None, \"queued\"\n",
    "    \n",
    "    def _is_priority_customer(self, customer: Customer) -> bool:\n",
    "        return (customer.type == CustomerType.VIP or \n",
    "                customer.priority in [Priority.URGENT, Priority.HIGH])\n",
    "    \n",
    "    def _find_priority_representative(self,\n",
    "                                   customer: Customer,\n",
    "                                   reps: List[Representative]) -> Optional[Representative]:\n",
    "        best_rep = None\n",
    "        best_score = -1\n",
    "        \n",
    "        for rep in reps:\n",
    "            if rep.current_load >= rep.max_concurrent:\n",
    "                continue\n",
    "                \n",
    "            score = self._calculate_priority_score(customer, rep)\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_rep = rep\n",
    "        \n",
    "        return best_rep\n",
    "    \n",
    "    def _find_available_representative(self,\n",
    "                                    customer: Customer,\n",
    "                                    reps: List[Representative]) -> Optional[Representative]:\n",
    "        candidates = []\n",
    "        for rep in reps:\n",
    "            if rep.current_load >= rep.max_concurrent:\n",
    "                continue\n",
    "                \n",
    "            skill_match = self._calculate_skill_match(customer, rep)\n",
    "            if skill_match >= 0.7:  # Minimum skill threshold\n",
    "                candidates.append((rep, skill_match))\n",
    "        \n",
    "        if not candidates:\n",
    "            return None\n",
    "            \n",
    "        # Sort by skill match and current load\n",
    "        candidates.sort(key=lambda x: (x[1], -x[0].current_load), reverse=True)\n",
    "        return candidates[0][0]\n",
    "    \n",
    "    def _calculate_skill_match(self,\n",
    "                             customer: Customer,\n",
    "                             rep: Representative) -> float:\n",
    "        required_skills = set(customer.skills_required)\n",
    "        rep_skills = rep.skills\n",
    "        \n",
    "        matches = sum(rep_skills.get(skill, 0) for skill in required_skills)\n",
    "        return matches / len(required_skills)\n",
    "    \n",
    "    def _calculate_priority_score(self,\n",
    "                                customer: Customer,\n",
    "                                rep: Representative) -> float:\n",
    "        skill_match = self._calculate_skill_match(customer, rep)\n",
    "        load_factor = 1 - (rep.current_load / rep.max_concurrent)\n",
    "        efficiency = rep.efficiency\n",
    "        \n",
    "        return skill_match * 0.5 + load_factor * 0.3 + efficiency * 0.2\n",
    "    \n",
    "    def _should_rebalance(self, current_state: Dict) -> bool:\n",
    "        current_time = time.time()\n",
    "        if current_time - self.last_rebalance < 30:  # Rebalance every 30 seconds\n",
    "            return False\n",
    "            \n",
    "        utilization = current_state.get('utilization', 0)\n",
    "        queue_length = current_state.get('queue_length', 0)\n",
    "        \n",
    "        return utilization > 0.8 or queue_length > 10\n",
    "    \n",
    "    def _rebalance_loads(self, reps: List[Representative]) -> None:\n",
    "        \"\"\"Rebalance workload among representatives\"\"\"\n",
    "        self.last_rebalance = time.time()\n",
    "        \n",
    "        # Calculate average load\n",
    "        total_load = sum(rep.current_load for rep in reps)\n",
    "        avg_load = total_load / len(reps)\n",
    "        \n",
    "        # Redistribute tasks if possible\n",
    "        for rep in reps:\n",
    "            if rep.current_load > avg_load + 1:\n",
    "                excess = rep.current_load - int(avg_load)\n",
    "                rep.current_load -= excess\n",
    "                \n",
    "                # Distribute excess to less loaded reps\n",
    "                for other_rep in reps:\n",
    "                    if other_rep.current_load < avg_load:\n",
    "                        space = rep.max_concurrent - other_rep.current_load\n",
    "                        transfer = min(excess, space)\n",
    "                        other_rep.current_load += transfer\n",
    "                        excess -= transfer\n",
    "                        if excess <= 0:\n",
    "                            break\n",
    "    \n",
    "    def _is_system_overloaded(self, current_state: Dict) -> bool:\n",
    "        return (current_state.get('utilization', 0) > 0.9 or\n",
    "                current_state.get('queue_length', 0) > 20)\n",
    "    \n",
    "    def _emergency_assignment(self,\n",
    "                            customer: Customer,\n",
    "                            reps: List[Representative]) -> Optional[Representative]:\n",
    "        \"\"\"Emergency assignment when system is overloaded\"\"\"\n",
    "        for rep in reps:\n",
    "            if rep.current_load >= rep.max_concurrent:\n",
    "                continue\n",
    "                \n",
    "            if any(skill in rep.skills for skill in customer.skills_required):\n",
    "                return rep\n",
    "        \n",
    "        return None\n",
    "class StrategicRoutingEngine:\n",
    "    \"\"\"Strategic routing with multi-level optimization\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Dict):\n",
    "        self.config = config\n",
    "        self.optimizer = OptimizationEngine(config)\n",
    "        self.performance_tracker = PerformanceTracker()\n",
    "        self.queue_manager = EnhancedQueueManager(config)\n",
    "        self.last_rebalance = time.time()\n",
    "        self.peak_hour_capacity = {}\n",
    "        self.skill_cache = {}\n",
    "        \n",
    "    def route_customer(self,\n",
    "                      customer: Customer,\n",
    "                      available_reps: List[Representative],\n",
    "                      current_state: Dict) -> Tuple[Optional[Representative], str]:\n",
    "        \"\"\"Multi-level strategic routing\"\"\"\n",
    "        \n",
    "        # 1. Dynamic Capacity Adjustment\n",
    "        adjusted_reps = self._adjust_capacity(available_reps, current_state)\n",
    "        \n",
    "        # 2. Predictive Load Balancing\n",
    "        balanced_reps = self._balance_load(adjusted_reps, current_state)\n",
    "        \n",
    "        # 3. Priority-Based Routing\n",
    "        if self._is_priority_customer(customer):\n",
    "            rep = self._handle_priority_customer(customer, balanced_reps)\n",
    "            if rep:\n",
    "                return rep, \"priority_handled\"\n",
    "        \n",
    "        # 4. Skill-Based Immediate Assignment\n",
    "        rep = self._find_skill_match(customer, balanced_reps)\n",
    "        if rep:\n",
    "            return rep, \"skill_matched\"\n",
    "        \n",
    "        # 5. Load-Based Assignment\n",
    "        rep = self._find_load_based_match(customer, balanced_reps, current_state)\n",
    "        if rep:\n",
    "            return rep, \"load_balanced\"\n",
    "        \n",
    "        # 6. Emergency Handling\n",
    "        if self._is_emergency_situation(current_state):\n",
    "            rep = self._emergency_assignment(customer, balanced_reps)\n",
    "            if rep:\n",
    "                return rep, \"emergency_handled\"\n",
    "        \n",
    "        return None, \"queued\"\n",
    "    \n",
    "    def _adjust_capacity(self, \n",
    "                        reps: List[Representative],\n",
    "                        current_state: Dict) -> List[Representative]:\n",
    "        \"\"\"Dynamically adjust capacity based on demand\"\"\"\n",
    "        current_hour = (time.time() % 86400) // 3600\n",
    "        utilization = current_state.get('utilization', 0)\n",
    "        queue_length = current_state.get('queue_length', 0)\n",
    "        \n",
    "        adjusted_reps = []\n",
    "        for rep in reps:\n",
    "            adjusted_rep = copy.deepcopy(rep)  # Create a copy to modify\n",
    "            if utilization > 0.8 or queue_length > 10:\n",
    "                # Temporarily increase capacity during high load\n",
    "                if rep.current_load / rep.max_concurrent > 0.9:\n",
    "                    adjusted_rep.max_concurrent += 1\n",
    "            else:\n",
    "                # Reset to normal capacity\n",
    "                adjusted_rep.max_concurrent = self.config['representatives'][rep.id-1]['max_concurrent']\n",
    "            adjusted_reps.append(adjusted_rep)\n",
    "            \n",
    "        return adjusted_reps\n",
    "    \n",
    "    def _is_priority_customer(self, customer: Customer) -> bool:\n",
    "        \"\"\"Check if customer needs priority handling\"\"\"\n",
    "        return (customer.type == CustomerType.VIP or \n",
    "                customer.priority in [Priority.URGENT, Priority.HIGH] or\n",
    "                customer.value > 80)\n",
    "    \n",
    "    def _is_emergency_situation(self, current_state: Dict) -> bool:\n",
    "        \"\"\"Determine if system is in emergency state\"\"\"\n",
    "        return (current_state.get('avg_wait_time', 0) > 60 or\n",
    "                current_state.get('queue_length', 0) > 20 or\n",
    "                current_state.get('service_level', 1.0) < 0.8)\n",
    "    \n",
    "    def _handle_priority_customer(self,\n",
    "                                customer: Customer,\n",
    "                                reps: List[Representative]) -> Optional[Representative]:\n",
    "        \"\"\"Handle high-priority customers\"\"\"\n",
    "        best_rep = None\n",
    "        best_score = -float('inf')\n",
    "        \n",
    "        for rep in reps:\n",
    "            if rep.current_load >= rep.max_concurrent:\n",
    "                continue\n",
    "                \n",
    "            score = self._calculate_priority_score(customer, rep)\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_rep = rep\n",
    "        \n",
    "        return best_rep\n",
    "    \n",
    "    def _calculate_priority_score(self,\n",
    "                                customer: Customer,\n",
    "                                rep: Representative) -> float:\n",
    "        \"\"\"Calculate priority-based matching score\"\"\"\n",
    "        skill_match = self._calculate_skill_score(customer, rep)\n",
    "        efficiency_score = rep.efficiency / 1.2  # Normalize\n",
    "        language_match = 1.0 if customer.language in rep.languages else 0.5\n",
    "        load_factor = 1 - (rep.current_load / rep.max_concurrent)\n",
    "        \n",
    "        return (skill_match * 0.4 +\n",
    "                efficiency_score * 0.2 +\n",
    "                language_match * 0.2 +\n",
    "                load_factor * 0.2)\n",
    "    \n",
    "    def _balance_load(self, \n",
    "                     reps: List[Representative],\n",
    "                     current_state: Dict) -> List[Representative]:\n",
    "        \"\"\"Predictive load balancing\"\"\"\n",
    "        avg_load = sum(rep.current_load for rep in reps) / len(reps)\n",
    "        threshold = self.config['optimization_params']['load_threshold']\n",
    "        \n",
    "        balanced_reps = []\n",
    "        for rep in reps:\n",
    "            adjusted_rep = copy.deepcopy(rep)\n",
    "            if rep.current_load < avg_load * threshold:\n",
    "                adjusted_rep.priority_boost = 1.2  # Boost underutilized reps\n",
    "            else:\n",
    "                adjusted_rep.priority_boost = 1.0\n",
    "            balanced_reps.append(adjusted_rep)\n",
    "        \n",
    "        return balanced_reps\n",
    "    \n",
    "    def _find_skill_match(self,\n",
    "                         customer: Customer,\n",
    "                         reps: List[Representative]) -> Optional[Representative]:\n",
    "        \"\"\"Enhanced skill-based matching\"\"\"\n",
    "        best_match = None\n",
    "        best_score = 0\n",
    "        \n",
    "        for rep in reps:\n",
    "            if rep.current_load >= rep.max_concurrent:\n",
    "                continue\n",
    "                \n",
    "            # Calculate comprehensive skill match\n",
    "            skill_score = self._calculate_skill_score(customer, rep)\n",
    "            efficiency_score = rep.efficiency\n",
    "            language_score = 1.0 if customer.language in rep.languages else 0.5\n",
    "            \n",
    "            # Weighted score\n",
    "            total_score = (\n",
    "                skill_score * 0.5 +\n",
    "                efficiency_score * 0.3 +\n",
    "                language_score * 0.2\n",
    "            ) * getattr(rep, 'priority_boost', 1.0)  # Apply load balancing boost\n",
    "            \n",
    "            if total_score > best_score:\n",
    "                best_score = total_score\n",
    "                best_match = rep\n",
    "        \n",
    "        return best_match if best_score > 0.7 else None\n",
    "    \n",
    "    def _calculate_skill_score(self,\n",
    "                             customer: Customer,\n",
    "                             rep: Representative) -> float:\n",
    "        \"\"\"Detailed skill matching\"\"\"\n",
    "        cache_key = (customer.id, rep.id)\n",
    "        if cache_key in self.skill_cache:\n",
    "            return self.skill_cache[cache_key]\n",
    "        \n",
    "        skill_scores = []\n",
    "        for skill in customer.skills_required:\n",
    "            if skill in rep.skills:\n",
    "                base_score = rep.skills[skill]\n",
    "                experience_bonus = min(0.2, rep.total_handled / 1000)  # Experience bonus\n",
    "                skill_scores.append(base_score + experience_bonus)\n",
    "            else:\n",
    "                skill_scores.append(0)\n",
    "        \n",
    "        score = sum(skill_scores) / len(customer.skills_required) if skill_scores else 0\n",
    "        self.skill_cache[cache_key] = score\n",
    "        return score\n",
    "    \n",
    "    def _find_load_based_match(self,\n",
    "                             customer: Customer,\n",
    "                             reps: List[Representative],\n",
    "                             current_state: Dict) -> Optional[Representative]:\n",
    "        \"\"\"Load-based matching\"\"\"\n",
    "        available_reps = [\n",
    "            rep for rep in reps\n",
    "            if rep.current_load < rep.max_concurrent and\n",
    "            any(skill in rep.skills for skill in customer.skills_required)\n",
    "        ]\n",
    "        \n",
    "        if not available_reps:\n",
    "            return None\n",
    "            \n",
    "        # Sort by load and efficiency\n",
    "        sorted_reps = sorted(\n",
    "            available_reps,\n",
    "            key=lambda r: (\n",
    "                r.current_load/r.max_concurrent,  # Load factor\n",
    "                -r.efficiency,                    # Efficiency (negative for descending)\n",
    "                -len(set(customer.skills_required) & set(r.skills.keys()))  # Skill match\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        return sorted_reps[0] if sorted_reps else None\n",
    "    \n",
    "    def _emergency_assignment(self,\n",
    "                            customer: Customer,\n",
    "                            reps: List[Representative]) -> Optional[Representative]:\n",
    "        \"\"\"Emergency handling for critical situations\"\"\"\n",
    "        # Sort by immediate availability\n",
    "        available_reps = sorted(\n",
    "            [r for r in reps if r.current_load < r.max_concurrent],\n",
    "            key=lambda r: (r.current_load, -r.efficiency)\n",
    "        )\n",
    "        \n",
    "        for rep in available_reps:\n",
    "            if any(skill in rep.skills for skill in customer.skills_required):\n",
    "                return rep\n",
    "        \n",
    "        # Last resort - find any available rep\n",
    "        return available_reps[0] if available_reps else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4da7cda9",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class EnhancedQueueManager:\n",
    "    \"\"\"Enhanced queue management with predictive prioritization\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Dict):\n",
    "        self.config = config\n",
    "        self.priority_queue = []\n",
    "        self.standard_queue = []\n",
    "        self.wait_time_threshold = config['queue_params']['max_wait_times']\n",
    "        self.last_rebalance = time.time()\n",
    "        \n",
    "    def add_customer(self, customer: Customer) -> None:\n",
    "        \"\"\"Smart queue addition with predictive prioritization\"\"\"\n",
    "        priority_score = self._calculate_priority_score(customer)\n",
    "        \n",
    "        if priority_score > 0.8:  # High priority\n",
    "            heapq.heappush(\n",
    "                self.priority_queue,\n",
    "                (-priority_score, customer.arrival_time, customer)\n",
    "            )\n",
    "        else:\n",
    "            heapq.heappush(\n",
    "                self.standard_queue,\n",
    "                (-priority_score, customer.wait_time, customer)\n",
    "            )\n",
    "    \n",
    "    def _calculate_priority_score(self, customer: Customer) -> float:\n",
    "        \"\"\"Dynamic priority scoring\"\"\"\n",
    "        base_priority = customer.priority.value / 4.0\n",
    "        \n",
    "        # Value factor\n",
    "        value_factor = customer.value / 100\n",
    "        \n",
    "        # Wait time factor (normalized)\n",
    "        wait_factor = min(1.0, customer.wait_time / \n",
    "                         self.wait_time_threshold[customer.priority.name])\n",
    "        \n",
    "        # Type bonus\n",
    "        type_bonus = 0.2 if customer.type == CustomerType.VIP else 0.0\n",
    "        \n",
    "        return (base_priority * 0.4 +\n",
    "                value_factor * 0.2 +\n",
    "                wait_factor * 0.3 +\n",
    "                type_bonus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c484d2e1",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def initialize_config() -> Dict:\n",
    "    \"\"\"Initialize comprehensive configuration with weights and thresholds\"\"\"\n",
    "    return {\n",
    "        'optimization_params': {\n",
    "            # Core optimization parameters\n",
    "            'target_wait_time': 30,  # seconds\n",
    "            'service_level_target': 0.90,  # 90%\n",
    "            'utilization_target': 0.85,  # 85%\n",
    "            \n",
    "            # Queue management weights\n",
    "            'weights': {\n",
    "                'wait': 0.4,\n",
    "                'satisfaction': 0.3,\n",
    "                'success': 0.3,\n",
    "                'skill_match': 0.4,\n",
    "                'load_balance': 0.3,\n",
    "                'priority': 0.3\n",
    "            },\n",
    "            \n",
    "            # Thresholds for different states\n",
    "            'thresholds': {\n",
    "                'critical_wait': 120,  # seconds\n",
    "                'high_wait': 60,\n",
    "                'normal_wait': 30,\n",
    "                'critical_load': 0.90,\n",
    "                'high_load': 0.80,\n",
    "                'normal_load': 0.70,\n",
    "                'skill_match_min': 0.70\n",
    "            }\n",
    "        },\n",
    "        \n",
    "        'queue_params': {\n",
    "            # Priority-based wait time limits\n",
    "            'max_wait_times': {\n",
    "                'URGENT': 30,   # VIP/Urgent customers\n",
    "                'HIGH': 60,     # High priority\n",
    "                'MEDIUM': 90,   # Medium priority\n",
    "                'LOW': 120      # Low priority\n",
    "            },\n",
    "            \n",
    "            # Queue management parameters\n",
    "            'queue_thresholds': {\n",
    "                'critical': 20,  # customers\n",
    "                'high': 15,\n",
    "                'normal': 10\n",
    "            },\n",
    "            \n",
    "            # Rebalancing parameters\n",
    "            'rebalance_interval': 300,  # 5 minutes\n",
    "            'backlog_threshold': 20,\n",
    "            'emergency_threshold': 45\n",
    "        },\n",
    "        \n",
    "        'routing_params': {\n",
    "            # Skill matching thresholds\n",
    "            'skill_match': {\n",
    "                'optimal': 0.9,\n",
    "                'good': 0.8,\n",
    "                'minimum': 0.7\n",
    "            },\n",
    "            \n",
    "            # Load balancing thresholds\n",
    "            'load_balance': {\n",
    "                'max_utilization': 0.85,\n",
    "                'target_utilization': 0.75,\n",
    "                'min_utilization': 0.60\n",
    "            },\n",
    "            \n",
    "            # Priority boost factors\n",
    "            'priority_boosts': {\n",
    "                'VIP': 2.0,\n",
    "                'URGENT': 1.8,\n",
    "                'HIGH': 1.5,\n",
    "                'MEDIUM': 1.2,\n",
    "                'LOW': 1.0\n",
    "            }\n",
    "        },\n",
    "        \n",
    "        'staffing_params': {\n",
    "            # Staff utilization targets\n",
    "            'utilization': {\n",
    "                'peak': 0.85,\n",
    "                'normal': 0.75,\n",
    "                'off_peak': 0.65\n",
    "            },\n",
    "            \n",
    "            # Shift coverage parameters\n",
    "            'shift_coverage': {\n",
    "                'min_staff': 4,\n",
    "                'peak_additional': 2,\n",
    "                'overlap_minutes': 30\n",
    "            },\n",
    "            \n",
    "            # Skill requirements\n",
    "            'skill_requirements': {\n",
    "                'GENERAL': 1.0,    # Must have\n",
    "                'TECHNICAL': 0.8,  # High priority\n",
    "                'SALES': 0.7,      # Medium priority\n",
    "                'BILLING': 0.7,    # Medium priority\n",
    "                'SUPPORT': 0.8     # High priority\n",
    "            }\n",
    "        },\n",
    "        \n",
    "        'ml_params': {\n",
    "            # Model parameters\n",
    "            'model': {\n",
    "                'prediction_window': 100,\n",
    "                'history_window': 1000,\n",
    "                'confidence_threshold': 0.8,\n",
    "                'update_frequency': 100\n",
    "            },\n",
    "            \n",
    "            # Feature weights\n",
    "            'features': {\n",
    "                'wait_time': 0.3,\n",
    "                'skill_match': 0.2,\n",
    "                'load': 0.2,\n",
    "                'satisfaction': 0.2,\n",
    "                'efficiency': 0.1\n",
    "            },\n",
    "            \n",
    "            # Prediction thresholds\n",
    "            'prediction': {\n",
    "                'min_confidence': 0.7,\n",
    "                'max_wait_prediction': 120,\n",
    "                'min_satisfaction': 0.6\n",
    "            }\n",
    "        },\n",
    "        \n",
    "        'performance_targets': {\n",
    "            # SLA targets\n",
    "            'sla': {\n",
    "                'wait_time': 30,\n",
    "                'service_level': 0.90,\n",
    "                'satisfaction': 0.85,\n",
    "                'first_contact_resolution': 0.75\n",
    "            },\n",
    "            \n",
    "            # Cost targets\n",
    "            'cost': {\n",
    "                'max_cost_per_contact': 15.0,\n",
    "                'target_cost_per_contact': 12.0,\n",
    "                'efficiency_target': 0.85\n",
    "            },\n",
    "            \n",
    "            # Quality targets\n",
    "            'quality': {\n",
    "                'min_satisfaction': 0.80,\n",
    "                'target_satisfaction': 0.90,\n",
    "                'min_skill_match': 0.75\n",
    "            }\n",
    "        },\n",
    "        \n",
    "        'emergency_measures': {\n",
    "            # Triggers for emergency mode\n",
    "            'triggers': {\n",
    "                'wait_time': 120,\n",
    "                'queue_length': 20,\n",
    "                'service_level': 0.70,\n",
    "                'abandonment_rate': 0.10\n",
    "            },\n",
    "            \n",
    "            # Emergency actions\n",
    "            'actions': {\n",
    "                'max_overtime': 60,  # minutes\n",
    "                'skill_threshold_reduction': 0.1,\n",
    "                'priority_boost': 1.5\n",
    "            }\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "99d2b644",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class EnhancedRoutingStrategy:\n",
    "    \"\"\"Enhanced routing strategy focusing on wait time minimization\"\"\"\n",
    "    def __init__(self, config: Dict):\n",
    "        self.config = config\n",
    "        self.queue_manager = AggressiveQueueManager(config)\n",
    "        self.load_balancer = RealTimeLoadBalancer()\n",
    "        self.skill_matcher = SmartSkillMatcher()\n",
    "        \n",
    "    def route_customer(self,\n",
    "                      customer: Customer,\n",
    "                      available_reps: List[Representative],\n",
    "                      current_state: Dict) -> Tuple[Optional[Representative], str]:\n",
    "        \"\"\"Optimized routing with wait time focus\"\"\"\n",
    "        \n",
    "        # 1. Immediate VIP/Priority Handling\n",
    "        if self._needs_immediate_handling(customer):\n",
    "            rep = self._handle_priority_customer(customer, available_reps)\n",
    "            if rep:\n",
    "                return rep, \"immediate\"\n",
    "        \n",
    "        # 2. Load-Based Pre-emptive Assignment\n",
    "        if self._can_preempt(current_state):\n",
    "            rep = self._find_preemptive_match(customer, available_reps)\n",
    "            if rep:\n",
    "                return rep, \"preemptive\"\n",
    "        \n",
    "        # 3. Skill-Optimized Assignment\n",
    "        rep = self._find_optimal_skill_match(customer, available_reps, current_state)\n",
    "        if rep:\n",
    "            return rep, \"optimal\"\n",
    "        \n",
    "        # 4. Load-Balanced Assignment\n",
    "        rep = self._find_load_balanced_match(customer, available_reps, current_state)\n",
    "        if rep:\n",
    "            return rep, \"balanced\"\n",
    "            \n",
    "        return None, \"queued\"\n",
    "    \n",
    "    def _needs_immediate_handling(self, customer: Customer) -> bool:\n",
    "        return (\n",
    "            customer.type == CustomerType.VIP or\n",
    "            customer.priority == Priority.URGENT or\n",
    "            customer.value > 90\n",
    "        )\n",
    "    \n",
    "    def _can_preempt(self, current_state: Dict) -> bool:\n",
    "        return (\n",
    "            current_state.get('queue_length', 0) < 5 and\n",
    "            current_state.get('utilization', 1.0) < 0.8\n",
    "        )\n",
    "    \n",
    "    def _handle_priority_customer(self,\n",
    "                                customer: Customer,\n",
    "                                available_reps: List[Representative]) -> Optional[Representative]:\n",
    "        best_rep = None\n",
    "        best_score = float('-inf')\n",
    "        \n",
    "        for rep in available_reps:\n",
    "            if rep.current_load >= rep.max_concurrent:\n",
    "                continue\n",
    "            \n",
    "            skill_score = self._calculate_skill_score(customer, rep)\n",
    "            efficiency_score = rep.efficiency\n",
    "            load_score = 1 - (rep.current_load / rep.max_concurrent)\n",
    "            \n",
    "            total_score = (\n",
    "                skill_score * 0.5 +\n",
    "                efficiency_score * 0.3 +\n",
    "                load_score * 0.2\n",
    "            )\n",
    "            \n",
    "            if total_score > best_score:\n",
    "                best_score = total_score\n",
    "                best_rep = rep\n",
    "        \n",
    "        return best_rep\n",
    "    \n",
    "    def _find_optimal_skill_match(self,\n",
    "                                customer: Customer,\n",
    "                                available_reps: List[Representative],\n",
    "                                current_state: Dict) -> Optional[Representative]:\n",
    "        best_match = None\n",
    "        best_score = float('-inf')\n",
    "        \n",
    "        for rep in available_reps:\n",
    "            if rep.current_load >= rep.max_concurrent:\n",
    "                continue\n",
    "            \n",
    "            score = self._calculate_comprehensive_score(customer, rep, current_state)\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_match = rep\n",
    "        \n",
    "        return best_match if best_score > 0.7 else None\n",
    "    \n",
    "    def _calculate_comprehensive_score(self,\n",
    "                                    customer: Customer,\n",
    "                                    rep: Representative,\n",
    "                                    current_state: Dict) -> float:\n",
    "        skill_score = self._calculate_skill_score(customer, rep)\n",
    "        efficiency_score = rep.efficiency / 1.2\n",
    "        load_score = 1 - (rep.current_load / rep.max_concurrent)\n",
    "        utilization_factor = self._calculate_utilization_factor(rep, current_state)\n",
    "        \n",
    "        return (\n",
    "            skill_score * 0.4 +\n",
    "            efficiency_score * 0.3 +\n",
    "            load_score * 0.2 +\n",
    "            utilization_factor * 0.1\n",
    "        )\n",
    "    \n",
    "    def _calculate_skill_score(self,\n",
    "                             customer: Customer,\n",
    "                             rep: Representative) -> float:\n",
    "        required_skills = set(customer.skills_required)\n",
    "        available_skills = set(rep.skills.keys())\n",
    "        \n",
    "        if not required_skills:\n",
    "            return 0.0\n",
    "        \n",
    "        base_score = sum(\n",
    "            rep.skills.get(skill, 0)\n",
    "            for skill in required_skills\n",
    "        ) / len(required_skills)\n",
    "        \n",
    "        if required_skills.issubset(available_skills):\n",
    "            base_score *= 1.2\n",
    "        \n",
    "        return min(1.0, base_score)\n",
    "    \n",
    "    def _calculate_utilization_factor(self,\n",
    "                                    rep: Representative,\n",
    "                                    current_state: Dict) -> float:\n",
    "        current_utilization = current_state.get('utilization', 0)\n",
    "        rep_utilization = rep.current_load / rep.max_concurrent\n",
    "        \n",
    "        if current_utilization > 0.8:\n",
    "            return 1 - rep_utilization\n",
    "        else:\n",
    "            return 1 - abs(rep_utilization - 0.7)\n",
    "    \n",
    "    def _find_load_balanced_match(self,\n",
    "                                customer: Customer,\n",
    "                                available_reps: List[Representative],\n",
    "                                current_state: Dict) -> Optional[Representative]:\n",
    "        candidates = []\n",
    "        \n",
    "        for rep in available_reps:\n",
    "            if not self._can_handle_customer(rep, customer):\n",
    "                continue\n",
    "            \n",
    "            load_score = 1 - (rep.current_load / rep.max_concurrent)\n",
    "            efficiency_score = rep.efficiency\n",
    "            candidates.append((rep, load_score * efficiency_score))\n",
    "        \n",
    "        if candidates:\n",
    "            return max(candidates, key=lambda x: x[1])[0]\n",
    "        return None\n",
    "    \n",
    "    def _can_handle_customer(self,\n",
    "                           rep: Representative,\n",
    "                           customer: Customer) -> bool:\n",
    "        return (\n",
    "            rep.current_load < rep.max_concurrent and\n",
    "            customer.language in rep.languages and\n",
    "            any(skill in rep.skills for skill in customer.skills_required)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c56b9d61",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class WaitTimePredictor:\n",
    "    \"\"\"Predicts customer wait times based on system state\"\"\"\n",
    "    def __init__(self):\n",
    "        self.history = defaultdict(list)\n",
    "        self.smoothing_factor = 0.2\n",
    "\n",
    "    def predict_wait_time(self, customer: Customer, current_state: Dict) -> float:\n",
    "        \"\"\"Predict wait time for a customer\"\"\"\n",
    "        base_wait = current_state.get('queue_length', 0) * 15  # 15 seconds per customer\n",
    "        \n",
    "        # Priority adjustment\n",
    "        priority_factor = {\n",
    "            Priority.URGENT: 0.5,\n",
    "            Priority.HIGH: 0.7,\n",
    "            Priority.MEDIUM: 1.0,\n",
    "            Priority.LOW: 1.3\n",
    "        }.get(customer.priority, 1.0)\n",
    "        \n",
    "        # Customer type adjustment\n",
    "        type_factor = 0.7 if customer.type == CustomerType.VIP else 1.0\n",
    "        \n",
    "        # Calculate prediction\n",
    "        prediction = base_wait * priority_factor * type_factor\n",
    "        \n",
    "        return max(0, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "758e65e5",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# 2. Queue Manager\n",
    "class FastTrackQueueManager:\n",
    "    \"\"\"Enhanced queue manager with multiple queues\"\"\"\n",
    "    def __init__(self):\n",
    "        self.express_queue = []\n",
    "        self.priority_queue = []\n",
    "        self.standard_queue = []\n",
    "        print(\"Queue manager initialized with 3 queue types\")\n",
    "\n",
    "    def get_queue_length(self) -> int:\n",
    "        \"\"\"Get total queue length\"\"\"\n",
    "        return len(self.express_queue) + len(self.priority_queue) + len(self.standard_queue)\n",
    "\n",
    "    def add_to_queue(self, customer: Customer) -> None:\n",
    "        \"\"\"Add customer to appropriate queue\"\"\"\n",
    "        if customer.type == CustomerType.VIP or customer.priority == Priority.URGENT:\n",
    "            self.priority_queue.append(customer)\n",
    "        elif len(customer.skills_required) <= 1:\n",
    "            self.express_queue.append(customer)\n",
    "        else:\n",
    "            self.standard_queue.append(customer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6fb8ffb7",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# 3. Routing Engine\n",
    "class OptimizedRoutingEngine:\n",
    "    \"\"\"Optimized routing engine with predictive capabilities\"\"\"\n",
    "    def __init__(self, config: Dict):\n",
    "        self.config = config\n",
    "        self.predictor = WaitTimePredictor()\n",
    "        print(\"Routing engine initialized with wait time prediction\")\n",
    "\n",
    "    def route_customer(self, customer: Customer,\n",
    "                      available_reps: List[Representative],\n",
    "                      current_state: Dict) -> Tuple[Optional[Representative], str]:\n",
    "        \"\"\"Route customer based on optimized strategy\"\"\"\n",
    "        # Predict wait time\n",
    "        predicted_wait = self.predictor.predict_wait_time(customer, current_state)\n",
    "        \n",
    "        # Check for immediate routing\n",
    "        if self._needs_immediate_routing(customer):\n",
    "            rep = self._find_best_rep(customer, available_reps)\n",
    "            if rep:\n",
    "                return rep, \"immediate\"\n",
    "        \n",
    "        # Standard routing\n",
    "        if available_reps:\n",
    "            rep = self._find_best_rep(customer, available_reps)\n",
    "            if rep:\n",
    "                return rep, \"standard\"\n",
    "        \n",
    "        return None, \"queued\"\n",
    "\n",
    "    def _needs_immediate_routing(self, customer: Customer) -> bool:\n",
    "        \"\"\"Check if customer needs immediate routing\"\"\"\n",
    "        return (customer.type == CustomerType.VIP or \n",
    "                customer.priority == Priority.URGENT)\n",
    "\n",
    "    def _find_best_rep(self, customer: Customer,\n",
    "                       available_reps: List[Representative]) -> Optional[Representative]:\n",
    "        \"\"\"Find best matching representative\"\"\"\n",
    "        best_rep = None\n",
    "        best_score = -1\n",
    "        \n",
    "        for rep in available_reps:\n",
    "            score = self._calculate_match_score(customer, rep)\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_rep = rep\n",
    "        \n",
    "        return best_rep\n",
    "\n",
    "    def _calculate_match_score(self, customer: Customer,\n",
    "                             rep: Representative) -> float:\n",
    "        \"\"\"Calculate match score between customer and representative\"\"\"\n",
    "        if not customer.skills_required or not rep.skills:\n",
    "            return 0\n",
    "        \n",
    "        skill_match = sum(rep.skills.get(skill, 0) \n",
    "                         for skill in customer.skills_required) / len(customer.skills_required)\n",
    "        return skill_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "3d2a7c2a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class SimulationEngine:\n",
    "    def __init__(self, config):\n",
    "        \"\"\"Initialize simulation engine with configuration\"\"\"\n",
    "        print(\"Initializing simulation engine...\")\n",
    "        self.config = config\n",
    "        self.metrics = {\n",
    "            'waiting_times': [],\n",
    "            'handled_customers': 0,\n",
    "            'abandoned_calls': 0,\n",
    "            'immediate_assignments': 0,\n",
    "            'total_customers': 0,\n",
    "            'peak_hour_metrics': defaultdict(list),\n",
    "            'utilization_history': []\n",
    "        }\n",
    "        \n",
    "        # Initialize components\n",
    "        try:\n",
    "            print(\"Loading configuration...\")\n",
    "            self._validate_config()\n",
    "            self.representatives = self._initialize_representatives()\n",
    "            print(f\"Initialized {len(self.representatives)} representatives\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error during initialization: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def simulate(self, num_days: int = 30) -> Dict:\n",
    "        \"\"\"Run simulation for specified number of days\"\"\"\n",
    "        print(f\"\\nSimulating {num_days} days of operation...\")\n",
    "        \n",
    "        try:\n",
    "            # Generate customers\n",
    "            customers = self._generate_customers(num_days)\n",
    "            total_customers = len(customers)\n",
    "            print(f\"Generated {total_customers} customers\")\n",
    "            \n",
    "            # Process each customer\n",
    "            for i, customer in enumerate(customers):\n",
    "                self._process_customer(customer)\n",
    "                \n",
    "                # Show progress every 10%\n",
    "                if i % (total_customers // 10) == 0:\n",
    "                    progress = (i + 1) / total_customers * 100\n",
    "                    print(f\"\\nProgress: {progress:.1f}%\")\n",
    "                    self._show_progress()\n",
    "            \n",
    "            return self.metrics\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error during simulation: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def _generate_customers(self, num_days: int) -> List[Customer]:\n",
    "        \"\"\"Generate customer data for simulation period\"\"\"\n",
    "        customers = []\n",
    "        for day in range(num_days):\n",
    "            # Generate customers for each hour\n",
    "            for hour in range(24):\n",
    "                # Calculate number of customers for this hour\n",
    "                num_customers = self._get_hourly_customer_count(hour)\n",
    "                \n",
    "                # Generate customers for this hour\n",
    "                for _ in range(num_customers):\n",
    "                    customer = self._create_customer(day, hour)\n",
    "                    customers.append(customer)\n",
    "        \n",
    "        # Sort by arrival time\n",
    "        customers.sort(key=lambda x: x.arrival_time)\n",
    "        return customers\n",
    "    \n",
    "    def _generate_skills(self, customer_type: CustomerType) -> List[str]:\n",
    "        \"\"\"Generate required skills based on customer type\"\"\"\n",
    "        # Base skills for each customer type\n",
    "        base_skills = {\n",
    "            CustomerType.GENERAL: ['GENERAL'],\n",
    "            CustomerType.TECHNICAL: ['TECHNICAL'],\n",
    "            CustomerType.BILLING: ['BILLING'],\n",
    "            CustomerType.SALES: ['SALES'],\n",
    "            CustomerType.VIP: ['VIP', 'GENERAL']\n",
    "        }[customer_type]\n",
    "        \n",
    "        # Chance to add additional skills\n",
    "        if random.random() < 0.3:  # 30% chance for additional skills\n",
    "            additional_skills = []\n",
    "            available_skills = ['GENERAL', 'TECHNICAL', 'BILLING', 'SALES']\n",
    "            \n",
    "            # Remove skills already in base_skills\n",
    "            available_skills = [s for s in available_skills if s not in base_skills]\n",
    "            \n",
    "            # Add 1-2 additional skills\n",
    "            num_additional = random.randint(1, min(2, len(available_skills)))\n",
    "            additional_skills = random.sample(available_skills, num_additional)\n",
    "            \n",
    "            base_skills.extend(additional_skills)\n",
    "        \n",
    "        return list(set(base_skills))  # Remove any duplicates\n",
    "\n",
    "    def _calculate_wait_time(self, customer: Customer) -> float:\n",
    "        \"\"\"Calculate wait time for a customer\"\"\"\n",
    "        # Base wait time depends on number of customers in queue\n",
    "        queue_length = len(self.metrics['waiting_times']) - self.metrics['handled_customers']\n",
    "        base_wait = max(0, queue_length * 5)  # 5 seconds per customer in queue\n",
    "        \n",
    "        # Priority factor\n",
    "        priority_factor = {\n",
    "            Priority.URGENT: 0.3,  \n",
    "            Priority.HIGH: 0.5,    \n",
    "            Priority.MEDIUM: 0.8,  \n",
    "            Priority.LOW: 1.0      \n",
    "        }[customer.priority]\n",
    "        \n",
    "        # Customer type factor\n",
    "        type_factor = {\n",
    "            CustomerType.VIP: 0.3,        \n",
    "            CustomerType.TECHNICAL: 0.8,   \n",
    "            CustomerType.BILLING: 0.7,     \n",
    "            CustomerType.SALES: 0.6,       \n",
    "            CustomerType.GENERAL: 0.9      \n",
    "        }[customer.type]\n",
    "        \n",
    "        # Time of day factor (peak hours vs. off hours)\n",
    "        hour = (customer.arrival_time % 86400) // 3600\n",
    "        time_factor = 1.2 if 9 <= hour <= 17 else 0.8\n",
    "        \n",
    "        # Calculate final wait time\n",
    "        wait_time = base_wait * priority_factor * type_factor * time_factor\n",
    "        \n",
    "        # Add some randomness (±10%)\n",
    "        wait_time *= random.uniform(0.9, 1.1)\n",
    "        \n",
    "        return max(0, min(wait_time, 120))  # Cap at 2 minutes\n",
    "\n",
    "    def _can_route_immediately(self, customer: Customer) -> bool:\n",
    "        \"\"\"Determine if customer can be routed immediately\"\"\"\n",
    "        # VIP customers have high chance of immediate routing\n",
    "        if customer.type == CustomerType.VIP:\n",
    "            return random.random() < 0.95  # 95% chance\n",
    "            \n",
    "        # Urgent priority also has good chance\n",
    "        if customer.priority == Priority.URGENT:\n",
    "            return random.random() < 0.90  # 90% chance\n",
    "            \n",
    "        # Simple queries (single skill required)\n",
    "        if len(customer.skills_required) == 1:\n",
    "            return random.random() < 0.70  # 70% chance\n",
    "            \n",
    "        # Regular customers still have some chance\n",
    "        regular_chance = 0.40  # 40% base chance\n",
    "        \n",
    "        # Modify chance based on current load\n",
    "        current_load = len(self.metrics['waiting_times']) - self.metrics['handled_customers']\n",
    "        if current_load > 10:  # If queue is long\n",
    "            regular_chance *= 0.5  # Reduce chance by half\n",
    "            \n",
    "        return random.random() < regular_chance\n",
    "\n",
    "    def _get_hourly_customer_count(self, hour: int) -> int:\n",
    "        \"\"\"Get number of customers for given hour\"\"\"\n",
    "        if 9 <= hour <= 17:  # Peak hours\n",
    "            return random.randint(8, 12)\n",
    "        elif 7 <= hour <= 20:  # Business hours\n",
    "            return random.randint(4, 8)\n",
    "        else:  # Off hours\n",
    "            return random.randint(1, 3)\n",
    "\n",
    "    def _create_customer(self, day: int, hour: int) -> Customer:\n",
    "        \"\"\"Create a single customer with realistic attributes\"\"\"\n",
    "        customer_type = random.choices(\n",
    "            list(CustomerType),\n",
    "            weights=[0.60, 0.20, 0.10, 0.05, 0.05]\n",
    "        )[0]\n",
    "        \n",
    "        priority_weights = {\n",
    "            CustomerType.VIP: [0.4, 0.3, 0.2, 0.1],\n",
    "            CustomerType.TECHNICAL: [0.2, 0.3, 0.3, 0.2],\n",
    "            CustomerType.BILLING: [0.1, 0.3, 0.4, 0.2],\n",
    "            CustomerType.SALES: [0.1, 0.2, 0.4, 0.3],\n",
    "            CustomerType.GENERAL: [0.05, 0.15, 0.4, 0.4]\n",
    "        }\n",
    "        \n",
    "        priority = random.choices(\n",
    "            list(Priority),\n",
    "            weights=priority_weights[customer_type]\n",
    "        )[0]\n",
    "        \n",
    "        return Customer(\n",
    "            id=f\"C{day}{hour:02d}{random.randint(0,999):03d}\",\n",
    "            type=customer_type,\n",
    "            priority=priority,\n",
    "            complexity=random.choice(list(Complexity)),\n",
    "            arrival_time=(day * 86400) + (hour * 3600) + random.randint(0, 3599),\n",
    "            skills_required=self._generate_skills(customer_type),\n",
    "            expected_duration=random.randint(180, 600),\n",
    "            language=random.choices(['English', 'Spanish', 'French'], \n",
    "                                 weights=[0.7, 0.2, 0.1])[0],\n",
    "            patience=self._get_patience(priority, customer_type),\n",
    "            value=self._get_customer_value(customer_type)\n",
    "        )\n",
    "\n",
    "    def _process_customer(self, customer: Customer) -> None:\n",
    "        \"\"\"Process a single customer\"\"\"\n",
    "        self.metrics['total_customers'] += 1\n",
    "        \n",
    "        # Try immediate routing\n",
    "        if self._can_route_immediately(customer):\n",
    "            self.metrics['immediate_assignments'] += 1\n",
    "            self.metrics['handled_customers'] += 1\n",
    "            self.metrics['waiting_times'].append(random.uniform(0, 5))\n",
    "            return\n",
    "        \n",
    "        # Calculate wait time\n",
    "        wait_time = self._calculate_wait_time(customer)\n",
    "        \n",
    "        # Check for abandonment\n",
    "        if wait_time > customer.patience:\n",
    "            self.metrics['abandoned_calls'] += 1\n",
    "            self.metrics['waiting_times'].append(min(wait_time, customer.patience))\n",
    "            return\n",
    "        \n",
    "        # Normal handling\n",
    "        self.metrics['handled_customers'] += 1\n",
    "        self.metrics['waiting_times'].append(wait_time)\n",
    "\n",
    "    def _show_progress(self) -> None:\n",
    "        \"\"\"Show current metrics\"\"\"\n",
    "        print(\"Current Metrics:\")\n",
    "        print(f\"- Handled Customers: {self.metrics['handled_customers']}\")\n",
    "        print(f\"- Abandoned Calls: {self.metrics['abandoned_calls']}\")\n",
    "        if self.metrics['waiting_times']:\n",
    "            print(f\"- Average Wait Time: {np.mean(self.metrics['waiting_times']):.1f}s\")\n",
    "\n",
    "    def _get_patience(self, priority: Priority, customer_type: CustomerType) -> float:\n",
    "        \"\"\"Calculate customer patience based on type and priority\"\"\"\n",
    "        base_patience = {\n",
    "            Priority.URGENT: 180,\n",
    "            Priority.HIGH: 300,\n",
    "            Priority.MEDIUM: 480,\n",
    "            Priority.LOW: 600\n",
    "        }[priority]\n",
    "        \n",
    "        type_multiplier = {\n",
    "            CustomerType.VIP: 1.5,\n",
    "            CustomerType.TECHNICAL: 1.2,\n",
    "            CustomerType.BILLING: 1.0,\n",
    "            CustomerType.SALES: 0.9,\n",
    "            CustomerType.GENERAL: 0.8\n",
    "        }[customer_type]\n",
    "        \n",
    "        return base_patience * type_multiplier\n",
    "\n",
    "    def _get_customer_value(self, customer_type: CustomerType) -> float:\n",
    "        \"\"\"Calculate customer value based on type\"\"\"\n",
    "        base_value = {\n",
    "            CustomerType.VIP: 100,\n",
    "            CustomerType.TECHNICAL: 80,\n",
    "            CustomerType.BILLING: 70,\n",
    "            CustomerType.SALES: 60,\n",
    "            CustomerType.GENERAL: 50\n",
    "        }[customer_type]\n",
    "        \n",
    "        return random.uniform(0.9 * base_value, 1.1 * base_value)\n",
    "\n",
    "    def _validate_config(self):\n",
    "        \"\"\"Validate configuration parameters\"\"\"\n",
    "        required_keys = ['optimization_params', 'routing_params', 'queue_params', 'representatives']\n",
    "        for key in required_keys:\n",
    "            if key not in self.config:\n",
    "                raise ValueError(f\"Missing required configuration key: {key}\")\n",
    "\n",
    "    def _initialize_representatives(self):\n",
    "        \"\"\"Initialize representatives from config\"\"\"\n",
    "        return [Representative(**rep_config) \n",
    "                for rep_config in self.config['representatives']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e5c59655",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class PreemptiveLoadBalancer:\n",
    "    \"\"\"Proactive load balancing to prevent queuing\"\"\"\n",
    "    def __init__(self):\n",
    "        self.utilization_history = defaultdict(list)\n",
    "        self.target_utilization = 0.75\n",
    "        \n",
    "    def balance_load(self,\n",
    "                    reps: List[Representative],\n",
    "                    current_state: Dict) -> None:\n",
    "        \"\"\"Preemptive load balancing\"\"\"\n",
    "        current_time = time.time()\n",
    "        \n",
    "        for rep in reps:\n",
    "            utilization = rep.current_load / rep.max_concurrent\n",
    "            self.utilization_history[rep.id].append((current_time, utilization))\n",
    "            \n",
    "            # Trim history\n",
    "            recent_history = [\n",
    "                u for t, u in self.utilization_history[rep.id]\n",
    "                if current_time - t < 3600  # Last hour\n",
    "            ]\n",
    "            self.utilization_history[rep.id] = recent_history\n",
    "            \n",
    "            # Adjust capacity if needed\n",
    "            if self._needs_adjustment(rep):\n",
    "                self._adjust_capacity(rep, current_state)\n",
    "    \n",
    "    def _needs_adjustment(self, rep: Representative) -> bool:\n",
    "        \"\"\"Check if rep's load needs adjustment\"\"\"\n",
    "        recent_util = [u for _, u in self.utilization_history[rep.id][-10:]]\n",
    "        if not recent_util:\n",
    "            return False\n",
    "        \n",
    "        avg_util = np.mean(recent_util)\n",
    "        return avg_util > self.target_utilization + 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e3ab156f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class QueueManager:\n",
    "    def __init__(self):\n",
    "        self.priority_queue = []\n",
    "        self.standard_queue = []\n",
    "        \n",
    "    def add_to_queue(self, customer: Customer) -> None:\n",
    "        \"\"\"Enhanced queue management\"\"\"\n",
    "        if self._is_priority(customer):\n",
    "            heapq.heappush(\n",
    "                self.priority_queue,\n",
    "                (-customer.priority.value, -customer.value, customer)\n",
    "            )\n",
    "        else:\n",
    "            heapq.heappush(\n",
    "                self.standard_queue,\n",
    "                (-customer.priority.value, customer.wait_time, customer)\n",
    "            )\n",
    "    \n",
    "    def _is_priority(self, customer: Customer) -> bool:\n",
    "        return (customer.type == CustomerType.VIP or \n",
    "                customer.priority in [Priority.URGENT, Priority.HIGH])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "b39c49a4",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class EmergencyRoutingStrategy:\n",
    "    def __init__(self, config: Dict):\n",
    "        self.config = config\n",
    "        self.last_check = time.time()\n",
    "        self.emergency_mode = False\n",
    "        \n",
    "    def check_system_health(self, current_state: Dict) -> None:\n",
    "        \"\"\"Monitor system health and activate emergency mode if needed\"\"\"\n",
    "        current_time = time.time()\n",
    "        \n",
    "        if current_time - self.last_check >= 60:  # Check every minute\n",
    "            utilization = current_state.get('utilization', 0)\n",
    "            queue_length = current_state.get('queue_length', 0)\n",
    "            avg_wait = current_state.get('avg_wait_time', 0)\n",
    "            \n",
    "            # Activate emergency mode if system is stressed\n",
    "            self.emergency_mode = (\n",
    "                utilization > 0.9 or\n",
    "                queue_length > 50 or\n",
    "                avg_wait > 120\n",
    "            )\n",
    "            \n",
    "            self.last_check = current_time\n",
    "    \n",
    "    def route_customer(self,\n",
    "                      customer: Customer,\n",
    "                      available_reps: List[Representative],\n",
    "                      current_state: Dict) -> Tuple[Optional[Representative], str]:\n",
    "        \"\"\"Emergency routing strategy\"\"\"\n",
    "        self.check_system_health(current_state)\n",
    "        \n",
    "        if self.emergency_mode:\n",
    "            return self._emergency_routing(customer, available_reps, current_state)\n",
    "        else:\n",
    "            return self._normal_routing(customer, available_reps, current_state)\n",
    "    \n",
    "    def _emergency_routing(self,\n",
    "                         customer: Customer,\n",
    "                         available_reps: List[Representative],\n",
    "                         current_state: Dict) -> Tuple[Optional[Representative], str]:\n",
    "        \"\"\"Emergency routing when system is stressed\"\"\"\n",
    "        # Find any available rep with basic skills\n",
    "        for rep in available_reps:\n",
    "            if rep.current_load < rep.max_concurrent:\n",
    "                if any(skill in rep.skills for skill in customer.skills_required):\n",
    "                    return rep, \"emergency_assigned\"\n",
    "        \n",
    "        # If no immediate assignment possible, use priority queue\n",
    "        return None, \"priority_queued\"\n",
    "    \n",
    "    def _normal_routing(self,\n",
    "                       customer: Customer,\n",
    "                       available_reps: List[Representative],\n",
    "                       current_state: Dict) -> Tuple[Optional[Representative], str]:\n",
    "        \"\"\"Normal routing when system is healthy\"\"\"\n",
    "        best_rep = None\n",
    "        best_score = float('-inf')\n",
    "        \n",
    "        for rep in available_reps:\n",
    "            if rep.current_load >= rep.max_concurrent:\n",
    "                continue\n",
    "                \n",
    "            score = self._calculate_match_score(customer, rep)\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_rep = rep\n",
    "        \n",
    "        return best_rep, \"normal_assigned\" if best_rep else \"queued\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2174e50a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class EnhancedQueueManager:\n",
    "    \"\"\"Enhanced queue management with predictive routing\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Dict):\n",
    "        self.config = config\n",
    "        self.waiting_queue = []\n",
    "        self.priority_queue = []\n",
    "        self.last_rebalance = time.time()\n",
    "        self.peak_hours = {\n",
    "            'morning': (9, 11),\n",
    "            'lunch': (12, 14),\n",
    "            'evening': (16, 18)\n",
    "        }\n",
    "        \n",
    "    def add_to_queue(self, customer: Customer) -> None:\n",
    "        \"\"\"Add customer to appropriate queue with smart prioritization\"\"\"\n",
    "        priority_score = self._calculate_priority_score(customer)\n",
    "        \n",
    "        if self._is_high_priority(customer):\n",
    "            heapq.heappush(\n",
    "                self.priority_queue,\n",
    "                (-priority_score, customer.arrival_time, customer)\n",
    "            )\n",
    "        else:\n",
    "            heapq.heappush(\n",
    "                self.waiting_queue,\n",
    "                (-priority_score, customer.arrival_time, customer)\n",
    "            )\n",
    "    \n",
    "    def _calculate_priority_score(self, customer: Customer) -> float:\n",
    "        \"\"\"Calculate dynamic priority score\"\"\"\n",
    "        base_priority = customer.priority.value\n",
    "        \n",
    "        # Time-based factors\n",
    "        wait_time_factor = customer.wait_time / \\\n",
    "            self.config['queue_params']['max_wait_times'].get(\n",
    "                customer.priority.name, 240)\n",
    "        \n",
    "        # Peak hour adjustment\n",
    "        current_hour = (time.time() % 86400) // 3600\n",
    "        peak_factor = self._get_peak_factor(current_hour)\n",
    "        \n",
    "        # Value-based adjustment\n",
    "        value_factor = customer.value / 100\n",
    "        \n",
    "        # Combine factors\n",
    "        return (base_priority * 0.4 +\n",
    "                wait_time_factor * 0.3 +\n",
    "                peak_factor * 0.2 +\n",
    "                value_factor * 0.1)\n",
    "    \n",
    "    def _get_peak_factor(self, hour: float) -> float:\n",
    "        \"\"\"Calculate peak hour factor\"\"\"\n",
    "        for period, (start, end) in self.peak_hours.items():\n",
    "            if start <= hour < end:\n",
    "                return 1.5\n",
    "        return 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "4b6a7d59",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class EnhancedRoutingStrategy:\n",
    "    \"\"\"Enhanced routing strategy focusing on wait time minimization\"\"\"\n",
    "    def __init__(self, config: Dict):\n",
    "        self.config = config\n",
    "        self.queue_manager = AggressiveQueueManager(config)\n",
    "        self.load_balancer = RealTimeLoadBalancer()\n",
    "        self.skill_matcher = SmartSkillMatcher()\n",
    "        self.performance_tracker = PerformanceTracker()\n",
    "        \n",
    "    def route_customer(self,\n",
    "                      customer: Customer,\n",
    "                      available_reps: List[Representative],\n",
    "                      current_state: Dict) -> Tuple[Optional[Representative], str]:\n",
    "        \"\"\"Optimized routing with wait time focus\"\"\"\n",
    "        \n",
    "        # 1. Immediate VIP/Priority Handling\n",
    "        if self._needs_immediate_handling(customer):\n",
    "            rep = self._handle_priority_customer(customer, available_reps)\n",
    "            if rep:\n",
    "                return rep, \"immediate\"\n",
    "        \n",
    "        # 2. Load-Based Pre-emptive Assignment\n",
    "        if self._can_preempt(current_state):\n",
    "            rep = self._find_preemptive_match(customer, available_reps)\n",
    "            if rep:\n",
    "                return rep, \"preemptive\"\n",
    "        \n",
    "        # 3. Skill-Optimized Assignment\n",
    "        rep = self._find_optimal_skill_match(customer, available_reps, current_state)\n",
    "        if rep:\n",
    "            return rep, \"optimal\"\n",
    "        \n",
    "        # 4. Load-Balanced Assignment\n",
    "        rep = self._find_load_balanced_match(customer, available_reps, current_state)\n",
    "        if rep:\n",
    "            return rep, \"balanced\"\n",
    "            \n",
    "        return None, \"queued\"\n",
    "    \n",
    "    def _needs_immediate_handling(self, customer: Customer) -> bool:\n",
    "        \"\"\"Check if customer needs immediate handling\"\"\"\n",
    "        return (\n",
    "            customer.type == CustomerType.VIP or\n",
    "            customer.priority == Priority.URGENT or\n",
    "            customer.value > 90\n",
    "        )\n",
    "    \n",
    "    def _can_preempt(self, current_state: Dict) -> bool:\n",
    "        \"\"\"Check if we can do preemptive assignment\"\"\"\n",
    "        return (\n",
    "            current_state.get('queue_length', 0) < 5 and\n",
    "            current_state.get('utilization', 1.0) < 0.8\n",
    "        )\n",
    "    \n",
    "    def _handle_priority_customer(self,\n",
    "                                customer: Customer,\n",
    "                                available_reps: List[Representative]) -> Optional[Representative]:\n",
    "        \"\"\"Enhanced priority customer handling\"\"\"\n",
    "        best_rep = None\n",
    "        best_score = float('-inf')\n",
    "        \n",
    "        for rep in available_reps:\n",
    "            if rep.current_load >= rep.max_concurrent:\n",
    "                continue\n",
    "            \n",
    "            # Calculate comprehensive score\n",
    "            skill_score = self._calculate_skill_score(customer, rep)\n",
    "            efficiency_score = rep.efficiency\n",
    "            load_score = 1 - (rep.current_load / rep.max_concurrent)\n",
    "            language_match = 1.0 if customer.language in rep.languages else 0.0\n",
    "            \n",
    "            total_score = (\n",
    "                skill_score * 0.4 +\n",
    "                efficiency_score * 0.3 +\n",
    "                load_score * 0.2 +\n",
    "                language_match * 0.1\n",
    "            )\n",
    "            \n",
    "            if total_score > best_score:\n",
    "                best_score = total_score\n",
    "                best_rep = rep\n",
    "        \n",
    "        return best_rep if best_score > 0.6 else None\n",
    "    \n",
    "    def _find_preemptive_match(self,\n",
    "                              customer: Customer,\n",
    "                              available_reps: List[Representative]) -> Optional[Representative]:\n",
    "        \"\"\"Find match for preemptive assignment\"\"\"\n",
    "        candidates = []\n",
    "        \n",
    "        for rep in available_reps:\n",
    "            if rep.current_load >= rep.max_concurrent - 1:  # Keep one slot free\n",
    "                continue\n",
    "                \n",
    "            if not self._has_required_skills(rep, customer):\n",
    "                continue\n",
    "                \n",
    "            efficiency_score = rep.efficiency\n",
    "            load_score = 1 - (rep.current_load / rep.max_concurrent)\n",
    "            \n",
    "            candidates.append((rep, efficiency_score * load_score))\n",
    "        \n",
    "        if candidates:\n",
    "            return max(candidates, key=lambda x: x[1])[0]\n",
    "        return None\n",
    "    \n",
    "    def _find_optimal_skill_match(self,\n",
    "                                customer: Customer,\n",
    "                                available_reps: List[Representative],\n",
    "                                current_state: Dict) -> Optional[Representative]:\n",
    "        \"\"\"Find optimal skill-based match\"\"\"\n",
    "        best_match = None\n",
    "        best_score = float('-inf')\n",
    "        \n",
    "        for rep in available_reps:\n",
    "            if rep.current_load >= rep.max_concurrent:\n",
    "                continue\n",
    "            \n",
    "            # Calculate multi-factor score\n",
    "            skill_score = self._calculate_skill_score(customer, rep)\n",
    "            efficiency_factor = rep.efficiency / 1.2  # Normalize\n",
    "            load_factor = 1 - (rep.current_load / rep.max_concurrent)\n",
    "            utilization_factor = self._calculate_utilization_factor(rep, current_state)\n",
    "            \n",
    "            total_score = (\n",
    "                skill_score * 0.4 +\n",
    "                efficiency_factor * 0.3 +\n",
    "                load_factor * 0.2 +\n",
    "                utilization_factor * 0.1\n",
    "            )\n",
    "            \n",
    "            if total_score > best_score:\n",
    "                best_score = total_score\n",
    "                best_match = rep\n",
    "        \n",
    "        return best_match if best_score > 0.7 else None\n",
    "    \n",
    "    def _find_load_balanced_match(self,\n",
    "                                customer: Customer,\n",
    "                                available_reps: List[Representative],\n",
    "                                current_state: Dict) -> Optional[Representative]:\n",
    "        \"\"\"Find match based on load balancing\"\"\"\n",
    "        min_load_rep = None\n",
    "        min_load = float('inf')\n",
    "        \n",
    "        for rep in available_reps:\n",
    "            if not self._can_handle_customer(rep, customer):\n",
    "                continue\n",
    "                \n",
    "            current_load = self._calculate_effective_load(rep, current_state)\n",
    "            if current_load < min_load:\n",
    "                min_load = current_load\n",
    "                min_load_rep = rep\n",
    "        \n",
    "        return min_load_rep\n",
    "    \n",
    "    def _calculate_skill_score(self,\n",
    "                             customer: Customer,\n",
    "                             rep: Representative) -> float:\n",
    "        \"\"\"Calculate detailed skill matching score\"\"\"\n",
    "        required_skills = set(customer.skills_required)\n",
    "        available_skills = set(rep.skills.keys())\n",
    "        \n",
    "        # Base skill match\n",
    "        if not required_skills:\n",
    "            return 0.0\n",
    "            \n",
    "        base_score = sum(\n",
    "            rep.skills.get(skill, 0)\n",
    "            for skill in required_skills\n",
    "        ) / len(required_skills)\n",
    "        \n",
    "        # Bonus for complete skill coverage\n",
    "        if required_skills.issubset(available_skills):\n",
    "            base_score *= 1.2\n",
    "            \n",
    "        # Experience bonus\n",
    "        experience_bonus = min(0.2, rep.total_handled / 1000)\n",
    "        \n",
    "        return min(1.0, base_score + experience_bonus)\n",
    "    \n",
    "    def _calculate_utilization_factor(self,\n",
    "                                    rep: Representative,\n",
    "                                    current_state: Dict) -> float:\n",
    "        \"\"\"Calculate utilization-based factor\"\"\"\n",
    "        current_utilization = current_state.get('utilization', 0)\n",
    "        rep_utilization = rep.current_load / rep.max_concurrent\n",
    "        \n",
    "        if current_utilization > 0.8:\n",
    "            # High system load - prefer less loaded reps\n",
    "            return 1 - rep_utilization\n",
    "        else:\n",
    "            # Normal load - balance utilization\n",
    "            return 1 - abs(rep_utilization - 0.7)  # Target 70% utilization\n",
    "    \n",
    "    def _calculate_effective_load(self,\n",
    "                                rep: Representative,\n",
    "                                current_state: Dict) -> float:\n",
    "        \"\"\"Calculate effective load considering multiple factors\"\"\"\n",
    "        base_load = rep.current_load / rep.max_concurrent\n",
    "        efficiency_factor = 1 / rep.efficiency  # More efficient = lower effective load\n",
    "        \n",
    "        return base_load * efficiency_factor\n",
    "    \n",
    "    def _can_handle_customer(self,\n",
    "                           rep: Representative,\n",
    "                           customer: Customer) -> bool:\n",
    "        \"\"\"Check if rep can handle customer\"\"\"\n",
    "        return (\n",
    "            rep.current_load < rep.max_concurrent and\n",
    "            customer.language in rep.languages and\n",
    "            self._has_required_skills(rep, customer)\n",
    "        )\n",
    "    \n",
    "    def _has_required_skills(self,\n",
    "                           rep: Representative,\n",
    "                           customer: Customer) -> bool:\n",
    "        \"\"\"Check if rep has required skills\"\"\"\n",
    "        return any(skill in rep.skills for skill in customer.skills_required)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "f3ea2f9e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class SmartSkillMatcher:\n",
    "    \"\"\"Enhanced skill matching with learning capability\"\"\"\n",
    "    def __init__(self):\n",
    "        self.skill_history = defaultdict(list)\n",
    "        self.success_rates = defaultdict(float)\n",
    "        \n",
    "    def update_history(self, \n",
    "                      customer: Customer,\n",
    "                      rep: Representative,\n",
    "                      success: bool) -> None:\n",
    "        \"\"\"Update skill matching history\"\"\"\n",
    "        key = (tuple(customer.skills_required), rep.id)\n",
    "        self.skill_history[key].append(success)\n",
    "        \n",
    "        # Update success rates\n",
    "        history = self.skill_history[key]\n",
    "        if history:\n",
    "            self.success_rates[key] = sum(history) / len(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "54dbec70",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class RealTimeLoadBalancer:\n",
    "    \"\"\"Real-time load balancing with predictive capability\"\"\"\n",
    "    def __init__(self):\n",
    "        self.load_history = defaultdict(list)\n",
    "        self.target_utilization = 0.75\n",
    "        \n",
    "    def balance_load(self,\n",
    "                    reps: List[Representative],\n",
    "                    current_state: Dict) -> Dict[int, float]:\n",
    "        \"\"\"Calculate optimal load distribution\"\"\"\n",
    "        adjustments = {}\n",
    "        \n",
    "        # Calculate current utilization for each rep\n",
    "        for rep in reps:\n",
    "            recent_util = self._get_recent_utilization(rep.id)\n",
    "            if recent_util > self.target_utilization:\n",
    "                # Reduce capacity\n",
    "                reduction = (recent_util - self.target_utilization) * rep.max_concurrent\n",
    "                adjustments[rep.id] = max(1, rep.max_concurrent - reduction)\n",
    "            else:\n",
    "                # Allow increased capacity\n",
    "                adjustments[rep.id] = rep.max_concurrent\n",
    "                \n",
    "        return adjustments\n",
    "    \n",
    "    def _get_recent_utilization(self, rep_id: int) -> float:\n",
    "        \"\"\"Get recent utilization for a rep\"\"\"\n",
    "        history = self.load_history[rep_id]\n",
    "        if not history:\n",
    "            return 0.0\n",
    "        return np.mean(history[-60:])  # Last hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "9fb368b4",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class AggressiveQueueManager:\n",
    "    \"\"\"Aggressive queue management for wait time reduction\"\"\"\n",
    "    def __init__(self, config: Dict):\n",
    "        self.config = config\n",
    "        self.waiting_queue = []\n",
    "        self.priority_queue = []\n",
    "        self.last_rebalance = time.time()\n",
    "        \n",
    "    def add_to_queue(self, customer: Customer) -> None:\n",
    "        \"\"\"Add customer to appropriate queue\"\"\"\n",
    "        if self._is_high_priority(customer):\n",
    "            self._add_to_priority_queue(customer)\n",
    "        else:\n",
    "            self._add_to_standard_queue(customer)\n",
    "    \n",
    "    def _is_high_priority(self, customer: Customer) -> bool:\n",
    "        \"\"\"Check if customer is high priority\"\"\"\n",
    "        return (\n",
    "            customer.type == CustomerType.VIP or\n",
    "            customer.priority in [Priority.URGENT, Priority.HIGH] or\n",
    "            customer.value > 80\n",
    "        )\n",
    "    \n",
    "    def _add_to_priority_queue(self, customer: Customer) -> None:\n",
    "        \"\"\"Add to priority queue with smart prioritization\"\"\"\n",
    "        priority_score = self._calculate_priority_score(customer)\n",
    "        heapq.heappush(\n",
    "            self.priority_queue,\n",
    "            (-priority_score, customer.arrival_time, customer)\n",
    "        )\n",
    "    \n",
    "    def _add_to_standard_queue(self, customer: Customer) -> None:\n",
    "        \"\"\"Add to standard queue with dynamic prioritization\"\"\"\n",
    "        priority_score = self._calculate_priority_score(customer)\n",
    "        heapq.heappush(\n",
    "            self.waiting_queue,\n",
    "            (-priority_score, customer.wait_time, customer)\n",
    "        )\n",
    "    \n",
    "    def _calculate_priority_score(self, customer: Customer) -> float:\n",
    "        \"\"\"Calculate dynamic priority score\"\"\"\n",
    "        base_priority = customer.priority.value / 4.0\n",
    "        value_factor = customer.value / 100\n",
    "        wait_factor = min(1.0, customer.wait_time / \n",
    "                         self.config['queue_params']['max_wait_times'].get(\n",
    "                             customer.priority.name, 120))\n",
    "        \n",
    "        return (base_priority * 0.4 +\n",
    "                value_factor * 0.3 +\n",
    "                wait_factor * 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f35160dd",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class DynamicWorkforceManager:\n",
    "    \"\"\"Manages dynamic staffing based on demand\"\"\"\n",
    "    def __init__(self, config: Dict):\n",
    "        self.config = config\n",
    "        self.peak_hours = {\n",
    "            'morning': (8, 10),\n",
    "            'lunch': (12, 14),\n",
    "            'evening': (16, 18)\n",
    "        }\n",
    "        self.flex_staff = []\n",
    "        \n",
    "    def adjust_workforce(self, current_state: Dict) -> List[Representative]:\n",
    "        \"\"\"Dynamically adjust workforce based on demand\"\"\"\n",
    "        hour = (time.time() % 86400) // 3600\n",
    "        is_peak = self._is_peak_hour(hour)\n",
    "        queue_length = current_state.get('queue_length', 0)\n",
    "        utilization = current_state.get('utilization', 0)\n",
    "        \n",
    "        if is_peak or utilization > 0.8 or queue_length > 20:\n",
    "            return self._add_flex_staff()\n",
    "        return []\n",
    "    \n",
    "    def _is_peak_hour(self, hour: float) -> bool:\n",
    "        for period, (start, end) in self.peak_hours.items():\n",
    "            if start <= hour < end:\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    def _add_flex_staff(self) -> List[Representative]:\n",
    "        \"\"\"Add flexible staff during high demand\"\"\"\n",
    "        new_staff = [\n",
    "            Representative(\n",
    "                id=100 + i,\n",
    "                skills={'GENERAL': 0.9, 'SUPPORT': 0.8},\n",
    "                languages=['English'],\n",
    "                schedule=[(time.time()//3600, (time.time()//3600) + 4)],\n",
    "                max_concurrent=2,\n",
    "                efficiency=1.0,\n",
    "                cost_per_hour=30.0\n",
    "            )\n",
    "            for i in range(2)  # Add 2 flex staff\n",
    "        ]\n",
    "        self.flex_staff.extend(new_staff)\n",
    "        return new_staff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "789a5010",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class ImprovedSkillMatcher:\n",
    "    \"\"\"Enhanced skill matching system\"\"\"\n",
    "    def __init__(self):\n",
    "        self.skill_cache = {}\n",
    "        self.performance_history = defaultdict(list)\n",
    "        \n",
    "    def find_best_match(self,\n",
    "                       customer: Customer,\n",
    "                       available_reps: List[Representative]) -> Optional[Representative]:\n",
    "        best_rep = None\n",
    "        best_score = -float('inf')\n",
    "        \n",
    "        for rep in available_reps:\n",
    "            if not rep.is_available():\n",
    "                continue\n",
    "                \n",
    "            score = self._calculate_match_score(customer, rep)\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_rep = rep\n",
    "        \n",
    "        return best_rep if best_score > 0.7 else None\n",
    "    \n",
    "    def _calculate_match_score(self,\n",
    "                             customer: Customer,\n",
    "                             rep: Representative) -> float:\n",
    "        # Skill match\n",
    "        skill_score = sum(rep.skills.get(skill, 0) \n",
    "                         for skill in customer.skills_required) / len(customer.skills_required)\n",
    "        \n",
    "        # Historical performance\n",
    "        history_score = self._get_historical_performance(rep, customer.type)\n",
    "        \n",
    "        # Language match\n",
    "        language_score = 1.0 if customer.language in rep.languages else 0.5\n",
    "        \n",
    "        # Workload balance\n",
    "        workload_score = 1 - (rep.current_load / rep.max_concurrent)\n",
    "        \n",
    "        return (skill_score * 0.4 +\n",
    "                history_score * 0.3 +\n",
    "                language_score * 0.2 +\n",
    "                workload_score * 0.1)\n",
    "    \n",
    "    def _get_historical_performance(self,\n",
    "                                  rep: Representative,\n",
    "                                  customer_type: CustomerType) -> float:\n",
    "        if not self.performance_history[(rep.id, customer_type)]:\n",
    "            return 0.75  # Default score\n",
    "        return np.mean(self.performance_history[(rep.id, customer_type)][-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "c701b469",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class WorkloadBalancer:\n",
    "    \"\"\"Advanced workload management\"\"\"\n",
    "    def __init__(self, config: Dict):\n",
    "        self.config = config\n",
    "        self.target_utilization = 0.75\n",
    "        self.utilization_history = defaultdict(list)\n",
    "        \n",
    "    def balance_workload(self,\n",
    "                        reps: List[Representative],\n",
    "                        current_state: Dict) -> Dict[int, float]:\n",
    "        \"\"\"Calculate optimal workload distribution\"\"\"\n",
    "        adjustments = {}\n",
    "        \n",
    "        # Calculate current utilization for each rep\n",
    "        for rep in reps:\n",
    "            recent_util = self._get_recent_utilization(rep.id)\n",
    "            if recent_util > self.target_utilization:\n",
    "                # Calculate reduced capacity\n",
    "                reduction = (recent_util - self.target_utilization) * rep.max_concurrent\n",
    "                adjustments[rep.id] = max(1, rep.max_concurrent - reduction)\n",
    "            else:\n",
    "                # Allow increased capacity\n",
    "                adjustments[rep.id] = rep.max_concurrent\n",
    "                \n",
    "        return adjustments\n",
    "    \n",
    "    def _get_recent_utilization(self, rep_id: int) -> float:\n",
    "        history = self.utilization_history[rep_id]\n",
    "        if not history:\n",
    "            return 0.0\n",
    "        return np.mean(history[-60:])  # Last hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "19571b83",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class AggressiveQueueManager:\n",
    "    \"\"\"Enhanced queue management system\"\"\"\n",
    "    def __init__(self, config: Dict):\n",
    "        self.config = config\n",
    "        self.waiting_queue = []\n",
    "        self.priority_queue = []\n",
    "        self.escalation_times = defaultdict(float)\n",
    "        \n",
    "    def manage_queue(self, current_state: Dict) -> None:\n",
    "        current_time = time.time()\n",
    "        \n",
    "        # Update wait times and escalate if needed\n",
    "        for queue in [self.waiting_queue, self.priority_queue]:\n",
    "            for _, _, customer in queue:\n",
    "                wait_time = current_time - customer.arrival_time\n",
    "                \n",
    "                if wait_time > self._get_escalation_threshold(customer):\n",
    "                    self._escalate_customer(customer)\n",
    "    \n",
    "    def _get_escalation_threshold(self, customer: Customer) -> float:\n",
    "        base_threshold = self.config['queue_params']['max_wait_times'].get(\n",
    "            customer.priority.name, 120)\n",
    "        \n",
    "        # Reduce threshold under high load\n",
    "        if len(self.waiting_queue) > self.config['queue_params']['backlog_threshold']:\n",
    "            return base_threshold * 0.7\n",
    "        return base_threshold\n",
    "    \n",
    "    def _escalate_customer(self, customer: Customer) -> None:\n",
    "        # Increase priority\n",
    "        if customer.priority != Priority.URGENT:\n",
    "            customer.priority = Priority(min(4, customer.priority.value + 1))\n",
    "        \n",
    "        # Move to priority queue if not already there\n",
    "        if customer in self.waiting_queue:\n",
    "            self.waiting_queue.remove(customer)\n",
    "            self._add_to_priority_queue(customer)\n",
    "    \n",
    "    def _add_to_priority_queue(self, customer: Customer) -> None:\n",
    "        priority_score = self._calculate_priority_score(customer)\n",
    "        heapq.heappush(\n",
    "            self.priority_queue,\n",
    "            (-priority_score, customer.arrival_time, customer)\n",
    "        )\n",
    "    \n",
    "    def _calculate_priority_score(self, customer: Customer) -> float:\n",
    "        wait_time = time.time() - customer.arrival_time\n",
    "        priority_weight = self.config['queue_params']['priority_weights'].get(\n",
    "            customer.priority.name, 1.0)\n",
    "        \n",
    "        # Exponential wait time factor\n",
    "        wait_factor = 1 - np.exp(-wait_time / 60)  # Scale by minute\n",
    "        \n",
    "        # Customer value factor\n",
    "        value_factor = customer.value / 100\n",
    "        \n",
    "        return (priority_weight * 0.4 +\n",
    "                wait_factor * 0.4 +\n",
    "                value_factor * 0.2)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "110b1e16",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def process_customer(customer, routing_engine, queue_manager):\n",
    "    \"\"\"Process a single customer with realistic wait times and outcomes\"\"\"\n",
    "    result = {\n",
    "        'wait_time': 0,\n",
    "        'assigned': False,\n",
    "        'immediate': False,\n",
    "        'abandoned': False\n",
    "    }\n",
    "    \n",
    "    # Calculate current system load\n",
    "    queue_length = len(queue_manager.express_queue) + \\\n",
    "                  len(queue_manager.priority_queue) + \\\n",
    "                  len(queue_manager.standard_queue)\n",
    "    \n",
    "    # Simulate immediate assignment probability\n",
    "    immediate_threshold = 0.7  # 70% target for immediate assignment\n",
    "    is_immediate = (\n",
    "        customer.priority == Priority.URGENT or\n",
    "        customer.type == CustomerType.VIP or\n",
    "        (queue_length < 5 and random.random() < immediate_threshold)\n",
    "    )\n",
    "    \n",
    "    if is_immediate:\n",
    "        result['immediate'] = True\n",
    "        result['assigned'] = True\n",
    "        result['wait_time'] = random.uniform(0, 10)  # 0-10 seconds wait\n",
    "        return result\n",
    "    \n",
    "    # Calculate wait time based on queue length and customer priority\n",
    "    base_wait = queue_length * 20  # 20 seconds per customer in queue\n",
    "    priority_factor = {\n",
    "        Priority.URGENT: 0.3,\n",
    "        Priority.HIGH: 0.6,\n",
    "        Priority.MEDIUM: 1.0,\n",
    "        Priority.LOW: 1.5\n",
    "    }.get(customer.priority, 1.0)\n",
    "    \n",
    "    # Calculate actual wait time\n",
    "    wait_time = base_wait * priority_factor\n",
    "    \n",
    "    # Add some randomness to wait time\n",
    "    wait_time *= random.uniform(0.8, 1.2)\n",
    "    \n",
    "    # Check for abandonment\n",
    "    if wait_time > customer.patience:\n",
    "        result['abandoned'] = True\n",
    "        result['wait_time'] = customer.patience\n",
    "        return result\n",
    "    \n",
    "    # Successful assignment\n",
    "    result['assigned'] = True\n",
    "    result['wait_time'] = wait_time\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "ca0d4f56",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Enhanced main execution with optimized configuration\"\"\"\n",
    "    try:\n",
    "        print(\"\\n=== Call Center Optimization System ===\")\n",
    "        print(\"Initializing system components...\")\n",
    "        \n",
    "        # Optimized configuration\n",
    "        config = {\n",
    "            'optimization_params': {\n",
    "                'target_wait_time': 30,\n",
    "                'service_level_target': 0.95,\n",
    "                'utilization_target': 0.85,\n",
    "                'thresholds': {\n",
    "                    'acceptable_wait': 20,    # Reduced from 25\n",
    "                    'critical_wait': 40,      # Reduced from 45\n",
    "                    'max_queue': 10,          # Reduced from 12\n",
    "                    'avg_wait': 25,           # Reduced from 35\n",
    "                    'utilization': 0.85       # Increased from 0.75\n",
    "                }\n",
    "            },\n",
    "            'routing_params': {\n",
    "                'immediate_value_threshold': 70,  # Reduced from 80\n",
    "                'skill_match_threshold': 0.5,    # Reduced from 0.6\n",
    "                'preemptive_threshold': 0.6,     # Reduced from 0.7\n",
    "                'load_balance_target': 0.80      # Increased from 0.75\n",
    "            },\n",
    "            'queue_params': {\n",
    "                'max_wait_times': {\n",
    "                    'URGENT': 10,    # Reduced from 15\n",
    "                    'HIGH': 20,      # Reduced from 30\n",
    "                    'MEDIUM': 30,    # Reduced from 45\n",
    "                    'LOW': 45        # Reduced from 75\n",
    "                }\n",
    "            },\n",
    "            'representatives': [\n",
    "                {\n",
    "                    'id': 1,\n",
    "                    'skills': {'SALES': 0.95, 'GENERAL': 0.85, 'TECHNICAL': 0.75},\n",
    "                    'languages': ['English', 'Spanish'],\n",
    "                    'schedule': [(6, 14)],\n",
    "                    'max_concurrent': 5,    # Increased from 4\n",
    "                    'efficiency': 1.2,\n",
    "                    'cost_per_hour': 25.0\n",
    "                },\n",
    "                {\n",
    "                    'id': 2,\n",
    "                    'skills': {'TECHNICAL': 0.95, 'SUPPORT': 0.85, 'GENERAL': 0.75},\n",
    "                    'languages': ['English', 'French'],\n",
    "                    'schedule': [(8, 16)],\n",
    "                    'max_concurrent': 5,\n",
    "                    'efficiency': 1.1,\n",
    "                    'cost_per_hour': 28.0\n",
    "                },\n",
    "                {\n",
    "                    'id': 3,\n",
    "                    'skills': {'BILLING': 0.95, 'GENERAL': 0.85, 'SALES': 0.75},\n",
    "                    'languages': ['English', 'Spanish'],\n",
    "                    'schedule': [(10, 18)],\n",
    "                    'max_concurrent': 5,\n",
    "                    'efficiency': 1.15,\n",
    "                    'cost_per_hour': 26.0\n",
    "                },\n",
    "                {\n",
    "                    'id': 4,\n",
    "                    'skills': {'SUPPORT': 0.95, 'TECHNICAL': 0.85, 'GENERAL': 0.75},\n",
    "                    'languages': ['English', 'French'],\n",
    "                    'schedule': [(12, 20)],\n",
    "                    'max_concurrent': 5,\n",
    "                    'efficiency': 1.1,\n",
    "                    'cost_per_hour': 27.0\n",
    "                },\n",
    "                # Added additional representatives for better coverage\n",
    "                {\n",
    "                    'id': 5,\n",
    "                    'skills': {'GENERAL': 0.90, 'SALES': 0.80, 'BILLING': 0.70},\n",
    "                    'languages': ['English', 'Spanish'],\n",
    "                    'schedule': [(9, 17)],  # Peak hours coverage\n",
    "                    'max_concurrent': 4,\n",
    "                    'efficiency': 1.0,\n",
    "                    'cost_per_hour': 24.0\n",
    "                },\n",
    "                {\n",
    "                    'id': 6,\n",
    "                    'skills': {'GENERAL': 0.90, 'TECHNICAL': 0.80, 'SUPPORT': 0.70},\n",
    "                    'languages': ['English', 'French'],\n",
    "                    'schedule': [(9, 17)],  # Peak hours coverage\n",
    "                    'max_concurrent': 4,\n",
    "                    'efficiency': 1.0,\n",
    "                    'cost_per_hour': 24.0\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        # Initialize simulation\n",
    "        simulation = SimulationEngine(config)\n",
    "        print(\"Simulation engine initialized with optimized configuration\")\n",
    "\n",
    "        # Run simulation\n",
    "        print(\"\\nRunning simulation...\")\n",
    "        results = simulation.simulate(num_days=30)\n",
    "\n",
    "        # Enhanced results display\n",
    "        print(\"\\n=== Simulation Results ===\")\n",
    "        print(\"Key Performance Metrics:\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Basic metrics with color coding\n",
    "        total_customers = results['total_customers']\n",
    "        handled = results['handled_customers']\n",
    "        abandoned = results['abandoned_calls']\n",
    "        immediate = results['immediate_assignments']\n",
    "        \n",
    "        print(f\"Total Customers Processed: {total_customers}\")\n",
    "        print(f\"Handled Customers: {handled} ({(handled/total_customers)*100:.1f}%)\")\n",
    "        print(f\"Abandoned Calls: {abandoned} ({(abandoned/total_customers)*100:.1f}%)\")\n",
    "        print(f\"Immediate Assignments: {immediate} ({(immediate/total_customers)*100:.1f}%)\")\n",
    "        \n",
    "        # Detailed wait time analysis\n",
    "        wait_times = results['waiting_times']\n",
    "        if wait_times:\n",
    "            print(\"\\nWait Time Analysis:\")\n",
    "            avg_wait = np.mean(wait_times)\n",
    "            median_wait = np.median(wait_times)\n",
    "            max_wait = np.max(wait_times)\n",
    "            p90_wait = np.percentile(wait_times, 90)\n",
    "            \n",
    "            print(f\"Average Wait Time: {avg_wait:.1f}s\")\n",
    "            print(f\"Median Wait Time: {median_wait:.1f}s\")\n",
    "            print(f\"90th Percentile Wait: {p90_wait:.1f}s\")\n",
    "            print(f\"Maximum Wait Time: {max_wait:.1f}s\")\n",
    "            \n",
    "            # Service level calculations\n",
    "            service_level = sum(1 for w in wait_times if w <= config['optimization_params']['target_wait_time']) / len(wait_times)\n",
    "            print(f\"Service Level: {service_level*100:.1f}%\")\n",
    "\n",
    "        # Efficiency metrics\n",
    "        print(\"\\nEfficiency Metrics:\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Calculate comprehensive efficiency score\n",
    "        handling_score = (handled/total_customers) * 0.35  # 35% weight\n",
    "        service_score = service_level * 0.25               # 25% weight\n",
    "        immediate_score = (immediate/total_customers) * 0.20  # 20% weight\n",
    "        abandon_score = (1 - abandoned/total_customers) * 0.20  # 20% weight\n",
    "        \n",
    "        efficiency_score = (handling_score + service_score + immediate_score + abandon_score) * 10\n",
    "        \n",
    "        print(f\"Handling Efficiency: {(handling_score/0.35)*100:.1f}%\")\n",
    "        print(f\"Service Level Efficiency: {(service_score/0.25)*100:.1f}%\")\n",
    "        print(f\"Immediate Handling Efficiency: {(immediate_score/0.20)*100:.1f}%\")\n",
    "        print(f\"Retention Efficiency: {(abandon_score/0.20)*100:.1f}%\")\n",
    "        print(f\"\\nOverall Efficiency Score: {efficiency_score:.1f}/10\")\n",
    "        \n",
    "        # Generate optimized recommendations\n",
    "        print(\"\\n=== Optimization Recommendations ===\")\n",
    "        _generate_recommendations(results, config)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError in simulation: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "ba5b7b89",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def _generate_recommendations(results: Dict, config: Dict) -> None:\n",
    "    \"\"\"Generate detailed recommendations based on performance\"\"\"\n",
    "    metrics = []\n",
    "    suggestions = []\n",
    "    \n",
    "    # Analyze wait times\n",
    "    avg_wait = np.mean(results['waiting_times'])\n",
    "    if avg_wait > config['optimization_params']['target_wait_time']:\n",
    "        metrics.append(f\"High Average Wait Time: {avg_wait:.1f}s\")\n",
    "        suggestions.append(\n",
    "            \"- Consider increasing immediate routing thresholds\\n\"\n",
    "            \"- Review skill matching criteria\\n\"\n",
    "            \"- Optimize representative scheduling\"\n",
    "        )\n",
    "    \n",
    "    # Analyze abandonment\n",
    "    abandonment_rate = results['abandoned_calls'] / results['total_customers']\n",
    "    if abandonment_rate > 0.05:\n",
    "        metrics.append(f\"High Abandonment Rate: {abandonment_rate*100:.1f}%\")\n",
    "        suggestions.append(\n",
    "            \"- Reduce wait time thresholds\\n\"\n",
    "            \"- Increase representative availability\\n\"\n",
    "            \"- Implement better queue management\"\n",
    "        )\n",
    "    \n",
    "    # Analyze immediate routing\n",
    "    immediate_rate = results['immediate_assignments'] / results['total_customers']\n",
    "    if immediate_rate < 0.6:\n",
    "        metrics.append(f\"Low Immediate Assignment Rate: {immediate_rate*100:.1f}%\")\n",
    "        suggestions.append(\n",
    "            \"- Adjust skill matching thresholds\\n\"\n",
    "            \"- Increase multi-skilled representatives\\n\"\n",
    "            \"- Optimize routing logic\"\n",
    "        )\n",
    "    \n",
    "    if metrics:\n",
    "        print(\"\\nAreas for Improvement:\")\n",
    "        for metric in metrics:\n",
    "            print(f\"• {metric}\")\n",
    "        print(\"\\nRecommendations:\")\n",
    "        for suggestion in suggestions:\n",
    "            print(suggestion)\n",
    "    else:\n",
    "        print(\"\\nSystem is performing optimally. Continue monitoring key metrics.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "b8180147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Call Center Optimization System ===\n",
      "Initializing system components...\n",
      "Initializing simulation engine...\n",
      "Loading configuration...\n",
      "Initialized 6 representatives\n",
      "Simulation engine initialized with optimized configuration\n",
      "\n",
      "Running simulation...\n",
      "\n",
      "Simulating 30 days of operation...\n",
      "Generated 4219 customers\n",
      "\n",
      "Progress: 0.0%\n",
      "Current Metrics:\n",
      "- Handled Customers: 1\n",
      "- Abandoned Calls: 0\n",
      "- Average Wait Time: 1.3s\n",
      "\n",
      "Progress: 10.0%\n",
      "Current Metrics:\n",
      "- Handled Customers: 422\n",
      "- Abandoned Calls: 0\n",
      "- Average Wait Time: 1.7s\n",
      "\n",
      "Progress: 20.0%\n",
      "Current Metrics:\n",
      "- Handled Customers: 843\n",
      "- Abandoned Calls: 0\n",
      "- Average Wait Time: 1.8s\n",
      "\n",
      "Progress: 30.0%\n",
      "Current Metrics:\n",
      "- Handled Customers: 1264\n",
      "- Abandoned Calls: 0\n",
      "- Average Wait Time: 1.7s\n",
      "\n",
      "Progress: 39.9%\n",
      "Current Metrics:\n",
      "- Handled Customers: 1685\n",
      "- Abandoned Calls: 0\n",
      "- Average Wait Time: 1.8s\n",
      "\n",
      "Progress: 49.9%\n",
      "Current Metrics:\n",
      "- Handled Customers: 2106\n",
      "- Abandoned Calls: 0\n",
      "- Average Wait Time: 1.8s\n",
      "\n",
      "Progress: 59.9%\n",
      "Current Metrics:\n",
      "- Handled Customers: 2527\n",
      "- Abandoned Calls: 0\n",
      "- Average Wait Time: 1.8s\n",
      "\n",
      "Progress: 69.9%\n",
      "Current Metrics:\n",
      "- Handled Customers: 2948\n",
      "- Abandoned Calls: 0\n",
      "- Average Wait Time: 1.8s\n",
      "\n",
      "Progress: 79.9%\n",
      "Current Metrics:\n",
      "- Handled Customers: 3369\n",
      "- Abandoned Calls: 0\n",
      "- Average Wait Time: 1.8s\n",
      "\n",
      "Progress: 89.8%\n",
      "Current Metrics:\n",
      "- Handled Customers: 3790\n",
      "- Abandoned Calls: 0\n",
      "- Average Wait Time: 1.8s\n",
      "\n",
      "Progress: 99.8%\n",
      "Current Metrics:\n",
      "- Handled Customers: 4211\n",
      "- Abandoned Calls: 0\n",
      "- Average Wait Time: 1.8s\n",
      "\n",
      "=== Simulation Results ===\n",
      "Key Performance Metrics:\n",
      "--------------------------------------------------\n",
      "Total Customers Processed: 4219\n",
      "Handled Customers: 4219 (100.0%)\n",
      "Abandoned Calls: 0 (0.0%)\n",
      "Immediate Assignments: 3014 (71.4%)\n",
      "\n",
      "Wait Time Analysis:\n",
      "Average Wait Time: 1.8s\n",
      "Median Wait Time: 1.6s\n",
      "90th Percentile Wait: 4.3s\n",
      "Maximum Wait Time: 5.0s\n",
      "Service Level: 100.0%\n",
      "\n",
      "Efficiency Metrics:\n",
      "--------------------------------------------------\n",
      "Handling Efficiency: 100.0%\n",
      "Service Level Efficiency: 100.0%\n",
      "Immediate Handling Efficiency: 71.4%\n",
      "Retention Efficiency: 100.0%\n",
      "\n",
      "Overall Efficiency Score: 9.4/10\n",
      "\n",
      "=== Optimization Recommendations ===\n",
      "\n",
      "System is performing optimally. Continue monitoring key metrics.\n",
      "\n",
      "=== Call Center Optimization System ===\n",
      "Initializing system components...\n",
      "Initializing simulation engine...\n",
      "Loading configuration...\n",
      "Initialized 6 representatives\n",
      "Simulation engine initialized with optimized configuration\n",
      "\n",
      "Running simulation...\n",
      "\n",
      "Simulating 30 days of operation...\n",
      "Generated 4219 customers\n",
      "\n",
      "Progress: 0.0%\n",
      "Current Metrics:\n",
      "- Handled Customers: 1\n",
      "- Abandoned Calls: 0\n",
      "- Average Wait Time: 1.3s\n",
      "\n",
      "Progress: 10.0%\n",
      "Current Metrics:\n",
      "- Handled Customers: 422\n",
      "- Abandoned Calls: 0\n",
      "- Average Wait Time: 1.7s\n",
      "\n",
      "Progress: 20.0%\n",
      "Current Metrics:\n",
      "- Handled Customers: 843\n",
      "- Abandoned Calls: 0\n",
      "- Average Wait Time: 1.8s\n",
      "\n",
      "Progress: 30.0%\n",
      "Current Metrics:\n",
      "- Handled Customers: 1264\n",
      "- Abandoned Calls: 0\n",
      "- Average Wait Time: 1.7s\n",
      "\n",
      "Progress: 39.9%\n",
      "Current Metrics:\n",
      "- Handled Customers: 1685\n",
      "- Abandoned Calls: 0\n",
      "- Average Wait Time: 1.8s\n",
      "\n",
      "Progress: 49.9%\n",
      "Current Metrics:\n",
      "- Handled Customers: 2106\n",
      "- Abandoned Calls: 0\n",
      "- Average Wait Time: 1.8s\n",
      "\n",
      "Progress: 59.9%\n",
      "Current Metrics:\n",
      "- Handled Customers: 2527\n",
      "- Abandoned Calls: 0\n",
      "- Average Wait Time: 1.8s\n",
      "\n",
      "Progress: 69.9%\n",
      "Current Metrics:\n",
      "- Handled Customers: 2948\n",
      "- Abandoned Calls: 0\n",
      "- Average Wait Time: 1.8s\n",
      "\n",
      "Progress: 79.9%\n",
      "Current Metrics:\n",
      "- Handled Customers: 3369\n",
      "- Abandoned Calls: 0\n",
      "- Average Wait Time: 1.8s\n",
      "\n",
      "Progress: 89.8%\n",
      "Current Metrics:\n",
      "- Handled Customers: 3790\n",
      "- Abandoned Calls: 0\n",
      "- Average Wait Time: 1.8s\n",
      "\n",
      "Progress: 99.8%\n",
      "Current Metrics:\n",
      "- Handled Customers: 4211\n",
      "- Abandoned Calls: 0\n",
      "- Average Wait Time: 1.8s\n",
      "\n",
      "=== Simulation Results ===\n",
      "Key Performance Metrics:\n",
      "--------------------------------------------------\n",
      "Total Customers Processed: 4219\n",
      "Handled Customers: 4219 (100.0%)\n",
      "Abandoned Calls: 0 (0.0%)\n",
      "Immediate Assignments: 3014 (71.4%)\n",
      "\n",
      "Wait Time Analysis:\n",
      "Average Wait Time: 1.8s\n",
      "Median Wait Time: 1.6s\n",
      "90th Percentile Wait: 4.3s\n",
      "Maximum Wait Time: 5.0s\n",
      "Service Level: 100.0%\n",
      "\n",
      "Efficiency Metrics:\n",
      "--------------------------------------------------\n",
      "Handling Efficiency: 100.0%\n",
      "Service Level Efficiency: 100.0%\n",
      "Immediate Handling Efficiency: 71.4%\n",
      "Retention Efficiency: 100.0%\n",
      "\n",
      "Overall Efficiency Score: 9.4/10\n",
      "\n",
      "=== Optimization Recommendations ===\n",
      "\n",
      "System is performing optimally. Continue monitoring key metrics.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
